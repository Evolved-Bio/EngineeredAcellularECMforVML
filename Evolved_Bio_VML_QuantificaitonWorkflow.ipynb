{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiEcrrIHbnJY"
      },
      "source": [
        "# **Step 1:** Loading high quality png files in the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Vo4j9n-hhK",
        "outputId": "83de1f84-3f37-4c2a-ea94-2f1e0687e04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cairosvg\n",
            "  Downloading CairoSVG-2.7.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting cairocffi (from cairosvg)\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting cssselect2 (from cairosvg)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cairosvg) (11.0.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from cairosvg) (1.4.0)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cairocffi->cairosvg) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n",
            "Downloading CairoSVG-2.7.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: cssselect2, cairocffi, cairosvg\n",
            "Successfully installed cairocffi-1.7.1 cairosvg-2.7.1 cssselect2-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install cairosvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz8-UpQ-Rqah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c471169c-4c82-4e97-bf19-5ccd01c4320a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Please enter the path to the folder containing your SVG files (e.g., /content/drive/MyDrive/your_folder): /content/drive/MyDrive/Codes (ML and whatnot)/VML image analysis/Temp\n",
            "Selected folder: Temp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing SVG files: 100%|██████████| 58/58 [06:45<00:00,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   Condition  Week Staining   Location  Animal processing_status\n",
            "0       Ctrl     2       HE  Treatment       8           SUCCESS\n",
            "1       Ctrl     2   Desmin  Treatment       8           SUCCESS\n",
            "2       Sham     2       HE  Treatment     130           SUCCESS\n",
            "3       Ctrl     2   Desmin  Treatment       9           SUCCESS\n",
            "4       Ctrl     2       HE  Treatment       9           SUCCESS\n",
            "5       Sham     2       HE  Treatment      13           SUCCESS\n",
            "6       Ctrl     2       HE  Treatment      80           SUCCESS\n",
            "7       Test     2       HE  Treatment       3           SUCCESS\n",
            "8       Ctrl     2   Desmin  Treatment      80           SUCCESS\n",
            "9       Test     2   Desmin  Treatment       3           SUCCESS\n",
            "10      Test     8       HE  Treatment      36           SUCCESS\n",
            "11      Ctrl     4       HE  Treatment      21           SUCCESS\n",
            "12      Sham     2   Desmin  Treatment     130           SUCCESS\n",
            "13      Sham     2   Desmin  Treatment      13           SUCCESS\n",
            "14      Test     4       HE  Treatment     180           SUCCESS\n",
            "15      Test     2       HE  Treatment       2           SUCCESS\n",
            "16      Test     2   Desmin  Treatment       2           SUCCESS\n",
            "17      Ctrl     4       HE  Treatment      23           SUCCESS\n",
            "18      Ctrl     4       HE  Treatment      22           SUCCESS\n",
            "19      Test     8       HE  Treatment      38           SUCCESS\n",
            "20      Ctrl     4   Desmin  Treatment      21           SUCCESS\n",
            "21      Test     8       HE  Treatment      33           SUCCESS\n",
            "22      Test     4       HE  Treatment      18           SUCCESS\n",
            "23      Test     8       HE  Treatment     380           SUCCESS\n",
            "24      Test     8       HE  Treatment     330           SUCCESS\n",
            "25      Ctrl     4   Desmin  Treatment      23           SUCCESS\n",
            "26      Test     4       HE  Treatment      16           SUCCESS\n",
            "27      Ctrl     4   Desmin  Treatment      22           SUCCESS\n",
            "28    Native     4       HE  Treatment      16           SUCCESS\n",
            "29      Test     4   Desmin  Treatment     180           SUCCESS\n",
            "30    Native     4       HE  Treatment     160           SUCCESS\n",
            "31    Native     8       HE  Treatment     330           SUCCESS\n",
            "32    Native     4   Desmin  Treatment    1700           SUCCESS\n",
            "33    Native     4       HE  Treatment    1600           SUCCESS\n",
            "34    Native     8   Desmin  Treatment    3800           SUCCESS\n",
            "35    Native     8   Desmin  Treatment     380           SUCCESS\n",
            "36    Native     8   Desmin  Treatment      38           SUCCESS\n",
            "37    Native     8       HE  Treatment      33           SUCCESS\n",
            "38      Test     2   Desmin  Treatment       4           SUCCESS\n",
            "39    Native     8       HE  Treatment    3300           SUCCESS\n",
            "40    Native     2       HE  Treatment     200           SUCCESS\n",
            "41    Native     2   Desmin  Treatment      20           SUCCESS\n",
            "42      Test     4   Desmin  Treatment      18           SUCCESS\n",
            "43    Native     2   Desmin  Treatment       2           SUCCESS\n",
            "44    Native     4   Desmin  Treatment     170           SUCCESS\n",
            "45    Native     2   Desmin  Treatment     200           SUCCESS\n",
            "46      Test     2       HE  Treatment       4           SUCCESS\n",
            "47    Native     2       HE  Treatment       2           SUCCESS\n",
            "48      Test     8   Desmin  Treatment      36           SUCCESS\n",
            "49      Test     8   Desmin  Treatment      33           SUCCESS\n",
            "50    Native     4   Desmin  Treatment      17           SUCCESS\n",
            "51      Test     8   Desmin  Treatment      38           SUCCESS\n",
            "52      Sham     2   Desmin  Treatment      11           SUCCESS\n",
            "53      Sham     2       HE  Treatment      11           SUCCESS\n",
            "54      Test     8   Desmin  Treatment     330           SUCCESS\n",
            "55      Test     4   Desmin  Treatment      16           SUCCESS\n",
            "56      Test     8   Desmin  Treatment     380           SUCCESS\n",
            "57    Native     2       HE  Treatment      20           SUCCESS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "import xml.etree.ElementTree as ET\n",
        "import cairosvg\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import io\n",
        "import tempfile\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from functools import partial\n",
        "import multiprocessing\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# All other functions remain exactly the same until process_svg_file\n",
        "def authenticate_drive():\n",
        "    \"\"\"Authenticate and create Drive API service.\"\"\"\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "        drive_service = build('drive', 'v3')\n",
        "        return drive_service\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Authentication failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def select_folder():\n",
        "    \"\"\"Prompt user to enter folder path.\"\"\"\n",
        "    from pathlib import Path\n",
        "    while True:\n",
        "        folder_path = input('Please enter the path to the folder containing your SVG files (e.g., /content/drive/MyDrive/your_folder): ')\n",
        "        folder_path = folder_path.strip()\n",
        "        if Path(folder_path).exists():\n",
        "            folder_name = os.path.basename(folder_path)\n",
        "            print(f\"Selected folder: {folder_name}\")\n",
        "            return {'id': None, 'name': folder_name, 'path': folder_path}\n",
        "        else:\n",
        "            print(\"Folder not found. Please try again.\")\n",
        "\n",
        "def get_files_from_folder(folder_info):\n",
        "    \"\"\"Get SVG files from the specified folder.\"\"\"\n",
        "    folder_path = folder_info['path']\n",
        "    svg_files = []\n",
        "    # Use list comprehension for faster file collection\n",
        "    svg_files = [\n",
        "        {'name': f, 'path': os.path.join(folder_path, f)}\n",
        "        for f in os.listdir(folder_path)\n",
        "        if f.lower().endswith('.svg')\n",
        "    ]\n",
        "    if not svg_files:\n",
        "        logger.error(\"No SVG files found in the selected folder.\")\n",
        "    return svg_files\n",
        "\n",
        "def download_svg_file(file_info):\n",
        "    \"\"\"Read an SVG file from the folder.\"\"\"\n",
        "    try:\n",
        "        with open(file_info['path'], 'rb') as f:\n",
        "            return io.BytesIO(f.read())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Compile regex pattern once\n",
        "FILENAME_PATTERN = re.compile(r'([^-]+)\\s*-\\s*Week\\s*(\\d+)\\s*-\\s*([^-]+)\\s*-\\s*([^-]+)\\s*-\\s*Animal\\s*(\\d+)')\n",
        "\n",
        "def extract_metadata(filename):\n",
        "    \"\"\"Extract metadata from filename.\"\"\"\n",
        "    match = FILENAME_PATTERN.match(filename)\n",
        "    if match:\n",
        "        condition, week, staining, location, animal = match.groups()\n",
        "        return {\n",
        "            'Filename': filename,\n",
        "            'Condition': condition.strip(),\n",
        "            'Week': int(week),\n",
        "            'Staining': staining.strip(),\n",
        "            'Location': location.strip(),\n",
        "            'Animal': int(animal)\n",
        "        }\n",
        "    else:\n",
        "        logger.warning(f\"Filename format error: {filename}\")\n",
        "        return None\n",
        "\n",
        "# Define namespaces at module level to avoid recreation\n",
        "NAMESPACES = {\n",
        "    'svg': 'http://www.w3.org/2000/svg',\n",
        "    'inkscape': 'http://www.inkscape.org/namespaces/inkscape',\n",
        "    'xlink': 'http://www.w3.org/1999/xlink'\n",
        "}\n",
        "\n",
        "def process_single_element(element_info, root, width, height, base_name, output_dir, metadata):\n",
        "    \"\"\"Process a single SVG element.\"\"\"\n",
        "    elem_type, id_type, id_value, subdir, suffix, needs_background, meta_key = element_info\n",
        "\n",
        "    stain_subdir = output_dir / subdir / metadata['Staining']\n",
        "    stain_subdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    new_root = ET.Element('svg')\n",
        "    new_root.set('xmlns', 'http://www.w3.org/2000/svg')\n",
        "    new_root.set('width', f'{width}px')\n",
        "    new_root.set('height', f'{height}px')\n",
        "    new_root.set('viewBox', f'0 0 {width} {height}')\n",
        "\n",
        "    if id_type == 'inkscape_label':\n",
        "        element = root.find(f\".//svg:{elem_type}[@{{{NAMESPACES['inkscape']}}}label='{id_value}']\", NAMESPACES)\n",
        "    else:\n",
        "        element = root.find(f\".//svg:{elem_type}[@id='{id_value}']\", NAMESPACES)\n",
        "\n",
        "    if element is not None:\n",
        "        if needs_background:\n",
        "            background = ET.SubElement(new_root, 'rect')\n",
        "            background.set('width', f'{width}px')\n",
        "            background.set('height', f'{height}px')\n",
        "            background.set('fill', 'white')\n",
        "            background.set('x', '0')\n",
        "            background.set('y', '0')\n",
        "\n",
        "        new_elem = ET.SubElement(new_root, elem_type)\n",
        "\n",
        "        if elem_type == 'image':\n",
        "            # Special handling for image elements\n",
        "            # Set dimensions first\n",
        "            new_elem.set('width', f'{width}px')\n",
        "            new_elem.set('height', f'{height}px')\n",
        "\n",
        "            # Explicitly set position\n",
        "            new_elem.set('x', '0')\n",
        "            new_elem.set('y', '0')\n",
        "\n",
        "            # Set preserveAspectRatio to none to ensure image fills the space\n",
        "            new_elem.set('preserveAspectRatio', 'none')\n",
        "\n",
        "            # Copy the image data and other attributes\n",
        "            for attr, value in element.attrib.items():\n",
        "                if attr == '{http://www.w3.org/1999/xlink}href':\n",
        "                    new_elem.set(attr, value)\n",
        "                elif attr not in ['width', 'height', 'x', 'y', 'preserveAspectRatio']:\n",
        "                    new_elem.set(attr, value)\n",
        "        else:\n",
        "            # Handle non-image elements (ROI and Grid)\n",
        "            for attr, value in element.attrib.items():\n",
        "                if attr in ['width', 'height']:\n",
        "                    new_elem.set(attr, f'{width}px')\n",
        "                else:\n",
        "                    new_elem.set(attr, value)\n",
        "\n",
        "            # Copy all child elements for non-image elements\n",
        "            for child in element:\n",
        "                new_elem.append(ET.fromstring(ET.tostring(child)))\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix='.svg', delete=False) as temp_svg:\n",
        "            tree = ET.ElementTree(new_root)\n",
        "            tree.write(temp_svg.name)\n",
        "\n",
        "        output_filename = f\"{base_name}{suffix}.png\"\n",
        "        output_path = stain_subdir / output_filename\n",
        "\n",
        "        try:\n",
        "            cairosvg.svg2png(\n",
        "                file_obj=open(temp_svg.name, 'rb'),\n",
        "                write_to=str(output_path),\n",
        "                output_width=width,\n",
        "                output_height=height,\n",
        "                dpi=250\n",
        "            )\n",
        "            result = {\n",
        "                'path': str(output_path.relative_to(output_dir)),\n",
        "                'dimensions': f\"{width}x{height}\",\n",
        "                'status': 'SUCCESS'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f'Error converting to PNG: {str(e)}')\n",
        "            result = {'path': \"CONVERSION_ERROR\", 'dimensions': None, 'status': 'ERROR'}\n",
        "\n",
        "        os.unlink(temp_svg.name)\n",
        "        return id_value, result\n",
        "    return id_value, {'path': \"MISSING\", 'dimensions': None, 'status': 'MISSING'}\n",
        "\n",
        "def process_svg_file(svg_content, filename, output_dir):\n",
        "    \"\"\"Process a single SVG file and extract layers.\"\"\"\n",
        "    base_name = Path(filename).stem\n",
        "    metadata = extract_metadata(base_name)\n",
        "    if not metadata:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        tree = ET.parse(io.BytesIO(svg_content.getvalue()))\n",
        "        root = tree.getroot()\n",
        "\n",
        "        original_image = root.find(f\".//svg:image[@id='image2']\", NAMESPACES)\n",
        "        if original_image is None:\n",
        "            logger.error(f\"Original image (image2) not found in {filename}. Cannot proceed.\")\n",
        "            return None\n",
        "\n",
        "        # Initialize width and height as None\n",
        "        width = height = None\n",
        "\n",
        "        # First try to get dimensions from viewBox as it's most reliable\n",
        "        viewbox = root.get('viewBox')\n",
        "        if viewbox:\n",
        "            try:\n",
        "                parts = viewbox.split()\n",
        "                if len(parts) == 4:\n",
        "                    width = int(float(parts[2]))\n",
        "                    height = int(float(parts[3]))\n",
        "            except (ValueError, TypeError, IndexError):\n",
        "                width = height = None\n",
        "\n",
        "        # If viewBox dimensions aren't available, fall back to image dimensions\n",
        "        if not width or not height:\n",
        "            def parse_dimension(value):\n",
        "                if value is None:\n",
        "                    return None\n",
        "                value = re.sub(r'[^0-9.]', '', str(value))\n",
        "                try:\n",
        "                    return int(float(value))\n",
        "                except (ValueError, TypeError):\n",
        "                    return None\n",
        "\n",
        "            width = parse_dimension(original_image.get('width'))\n",
        "            height = parse_dimension(original_image.get('height'))\n",
        "\n",
        "            # Last resort: try root SVG dimensions\n",
        "            if width is None:\n",
        "                width = parse_dimension(root.get('width'))\n",
        "            if height is None:\n",
        "                height = parse_dimension(root.get('height'))\n",
        "\n",
        "        if not width or not height:\n",
        "            logger.error(f\"Could not determine valid dimensions for {filename}\")\n",
        "            return None\n",
        "\n",
        "        # Rest of the function remains exactly the same\n",
        "        elements_to_process = [\n",
        "            ('image', 'id', 'image2', 'Original-Images', '', False, 'OriginalPath'),\n",
        "            ('g', 'id', 'g3', 'ROI-Images', ' (ROI)', True, 'ROIPath'),\n",
        "            ('g', 'inkscape_label', 'Grids', 'Grid-Images', ' (Grid)', True, 'GridPath')\n",
        "        ]\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            process_func = partial(process_single_element, root=root, width=width, height=height,\n",
        "                                 base_name=base_name, output_dir=output_dir, metadata=metadata)\n",
        "            futures = [executor.submit(process_func, element_info) for element_info in elements_to_process]\n",
        "\n",
        "            results = {}\n",
        "            for future in as_completed(futures):\n",
        "                id_value, result = future.result()\n",
        "                results[id_value] = result\n",
        "\n",
        "        # Update metadata with results\n",
        "        all_missing = True\n",
        "        missing_elements = []\n",
        "\n",
        "        for elem_info in elements_to_process:\n",
        "            id_value = elem_info[2]\n",
        "            meta_key = elem_info[6]\n",
        "            result = results[id_value]\n",
        "\n",
        "            if result['status'] != 'MISSING':\n",
        "                all_missing = False\n",
        "            else:\n",
        "                missing_elements.append(id_value)\n",
        "\n",
        "            metadata[meta_key] = result['path']\n",
        "            if result['dimensions']:\n",
        "                metadata[f'{meta_key}_dimensions'] = result['dimensions']\n",
        "\n",
        "        if all_missing:\n",
        "            metadata['processing_status'] = \"NO_ELEMENTS_FOUND\"\n",
        "        elif missing_elements:\n",
        "            metadata['processing_status'] = f\"MISSING_ELEMENTS: {', '.join(missing_elements)}\"\n",
        "        else:\n",
        "            metadata['processing_status'] = \"SUCCESS\"\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error processing {filename}: {str(e)}')\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process SVG files from Google Drive.\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        logger.info(\"Drive already mounted\")\n",
        "\n",
        "    folder_info = select_folder()\n",
        "    if not folder_info:\n",
        "        logger.error(\"No folder was selected\")\n",
        "        return\n",
        "\n",
        "    os.environ['PROCESSED_FOLDER_PATH'] = folder_info['path']\n",
        "    output_dir = Path(folder_info['path'])\n",
        "    logger.info(f\"Selected folder: {folder_info['name']}\")\n",
        "\n",
        "    svg_files = get_files_from_folder(folder_info)\n",
        "    if not svg_files:\n",
        "        logger.error(\"No SVG files found for processing!\")\n",
        "        return\n",
        "\n",
        "    metadata_list = []\n",
        "    files_with_issues = []\n",
        "\n",
        "    # Calculate optimal number of workers based on CPU cores\n",
        "    max_workers = min(32, (multiprocessing.cpu_count() * 2))\n",
        "\n",
        "    # Process files in parallel using ThreadPoolExecutor\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_file = {\n",
        "            executor.submit(download_svg_file, file): file\n",
        "            for file in svg_files\n",
        "        }\n",
        "\n",
        "        # Use tqdm to show progress\n",
        "        with tqdm(total=len(svg_files), desc=\"Processing SVG files\") as pbar:\n",
        "            for future in as_completed(future_to_file):\n",
        "                file = future_to_file[future]\n",
        "                svg_content = future.result()\n",
        "\n",
        "                if svg_content:\n",
        "                    metadata = process_svg_file(svg_content, file['name'], output_dir)\n",
        "                    if metadata:\n",
        "                        metadata_list.append(metadata)\n",
        "                        if metadata['processing_status'] != \"SUCCESS\":\n",
        "                            files_with_issues.append((file['name'], metadata['processing_status']))\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    if metadata_list:\n",
        "        df = pd.DataFrame(metadata_list)\n",
        "        metadata_path = output_dir / 'metadata.csv'\n",
        "        df.to_csv(metadata_path, index=False)\n",
        "        logger.info(f\"\\nMetadata saved to metadata.csv in {folder_info['name']}\")\n",
        "\n",
        "        logger.info(\"\\nProcessing Summary:\")\n",
        "        logger.info(f\"Total files processed: {len(metadata_list)}\")\n",
        "        logger.info(f\"Files with all components: {len(metadata_list) - len(files_with_issues)}\")\n",
        "\n",
        "        if files_with_issues:\n",
        "            logger.info(\"\\nFiles with missing components:\")\n",
        "            for filename, status in files_with_issues:\n",
        "                logger.info(f\"- {filename}: {status}\")\n",
        "\n",
        "        logger.info(\"\\nProcessed Files Summary:\")\n",
        "        print(\"\")\n",
        "        display_columns = ['Condition', 'Week', 'Staining', 'Location', 'Animal', 'processing_status']\n",
        "        print(df[display_columns])\n",
        "\n",
        "        logger.info(f\"\\nProcessing complete. Folder path stored for next script: {folder_info['path']}\")\n",
        "    else:\n",
        "        logger.warning(\"No files were successfully processed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSPzR2tELvNa"
      },
      "source": [
        "# **Step 2:** Prosessing images using ROI and Grid lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hdY8Qw6jSzz"
      },
      "source": [
        "Deleting anything outside ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qaz_-6y0xvCf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import re\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def log_memory():\n",
        "    \"\"\"Log current memory usage.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "    logger.info(f\"Memory usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "def sanitize_filename(name):\n",
        "    \"\"\"Sanitize filename by removing invalid characters.\"\"\"\n",
        "    return re.sub(r'[^\\w\\-_\\. ]', '_', name)\n",
        "\n",
        "def create_roi_mask(roi_image):\n",
        "    \"\"\"Create mask from ROI image with black contour on white background.\"\"\"\n",
        "    try:\n",
        "        # Convert to grayscale if needed\n",
        "        if len(roi_image.shape) == 3:\n",
        "            roi_gray = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "            del roi_image  # Immediate cleanup\n",
        "        else:\n",
        "            roi_gray = roi_image.copy()\n",
        "            del roi_image\n",
        "\n",
        "        # Use adaptive thresholding for better results\n",
        "        binary = cv2.adaptiveThreshold(\n",
        "            roi_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY_INV, 11, 2\n",
        "        )\n",
        "        del roi_gray  # Cleanup grayscale image\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        mask = np.zeros_like(binary)\n",
        "        del binary  # Cleanup binary image\n",
        "\n",
        "        if contours:\n",
        "            cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
        "        else:\n",
        "            logger.warning(\"No contours found in ROI image.\")\n",
        "\n",
        "        del contours  # Cleanup contours\n",
        "        return mask\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in create_roi_mask: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def apply_roi_mask(original_image, mask):\n",
        "    \"\"\"Apply ROI mask to original image.\"\"\"\n",
        "    try:\n",
        "        # Ensure mask is binary\n",
        "        mask_binary = mask.astype(bool)\n",
        "        del mask  # Original mask no longer needed\n",
        "\n",
        "        # Create result array\n",
        "        result = np.full_like(original_image, 255)\n",
        "\n",
        "        # Apply mask\n",
        "        result[mask_binary] = original_image[mask_binary]\n",
        "        del mask_binary  # Cleanup mask\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in apply_roi_mask: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def visualize_roi_process(roi_image, original_image, result, output_path):\n",
        "    \"\"\"Visualize original, ROI, and masked result and save to file.\"\"\"\n",
        "    try:\n",
        "        plt.close('all')  # Ensure all previous plots are closed\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # Process one image at a time\n",
        "        # Original image\n",
        "        rgb_original = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "        axs[0].imshow(rgb_original)\n",
        "        axs[0].set_title('Original Image')\n",
        "        axs[0].axis('off')\n",
        "        del rgb_original\n",
        "\n",
        "        # ROI contour\n",
        "        rgb_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
        "        axs[1].imshow(rgb_roi)\n",
        "        axs[1].set_title('ROI Contour')\n",
        "        axs[1].axis('off')\n",
        "        del rgb_roi\n",
        "\n",
        "        # Final masked result\n",
        "        rgb_result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "        axs[2].imshow(rgb_result)\n",
        "        axs[2].set_title('Masked Result')\n",
        "        axs[2].axis('off')\n",
        "        del rgb_result\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(str(output_path), bbox_inches='tight', dpi=250)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in visualize_roi_process: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "def process_single_image(orig_path, roi_path, filename):\n",
        "    \"\"\"Process a single image pair.\"\"\"\n",
        "    try:\n",
        "        # Read images one at a time\n",
        "        orig_img = cv2.imread(str(orig_path))\n",
        "        if orig_img is None:\n",
        "            logger.error(f\"Failed to read original image: {filename}\")\n",
        "            return None, None, None\n",
        "\n",
        "        roi_img = cv2.imread(str(roi_path))\n",
        "        if roi_img is None:\n",
        "            logger.error(f\"Failed to read ROI image: {filename}\")\n",
        "            del orig_img\n",
        "            return None, None, None\n",
        "\n",
        "        # Check sizes\n",
        "        if orig_img.shape != roi_img.shape:\n",
        "            logger.error(f\"Size mismatch: Original {orig_img.shape} vs ROI {roi_img.shape} for {filename}\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Create mask\n",
        "        mask = create_roi_mask(roi_img.copy())\n",
        "        if mask is None:\n",
        "            return None, None, None\n",
        "\n",
        "        # Apply mask\n",
        "        result = apply_roi_mask(orig_img, mask)\n",
        "        del mask  # Cleanup mask\n",
        "\n",
        "        if result is None:\n",
        "            return None, None, None\n",
        "\n",
        "        return orig_img, roi_img, result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in process_single_image for {filename}: {e}\")\n",
        "        return None, None, None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def process_staining_group(stain_df, base_path, output_dir, vis_dir, metadata_df):\n",
        "    \"\"\"Process a group of images with the same staining.\"\"\"\n",
        "    try:\n",
        "        for idx in tqdm(stain_df.index, desc=f\"Processing {stain_df.iloc[0]['Staining']} images\"):\n",
        "            row = stain_df.loc[idx]\n",
        "\n",
        "            # Check if files exist\n",
        "            orig_path = base_path / row['OriginalPath']\n",
        "            roi_path = base_path / row['ROIPath']\n",
        "\n",
        "            if not (orig_path.exists() and roi_path.exists()):\n",
        "                logger.warning(f\"Missing files for {row['Filename']}\")\n",
        "                continue\n",
        "\n",
        "            # Process single image pair\n",
        "            orig_img, roi_img, result = process_single_image(orig_path, roi_path, row['Filename'])\n",
        "\n",
        "            if result is not None:\n",
        "                try:\n",
        "                    # Create output filename\n",
        "                    output_name = f\"{row['Condition']}_Week{row['Week']}_{row['Staining']}_{row['Location']}_Animal{row['Animal']}_masked.png\"\n",
        "                    output_name = sanitize_filename(output_name)\n",
        "                    output_path = output_dir / output_name\n",
        "\n",
        "                    # Save result\n",
        "                    cv2.imwrite(str(output_path), result)\n",
        "\n",
        "                    # Create and save visualization\n",
        "                    vis_output_name = output_name.replace('.png', '_visualization.png')\n",
        "                    vis_output_path = vis_dir / vis_output_name\n",
        "                    visualize_roi_process(roi_img, orig_img, result, vis_output_path)\n",
        "\n",
        "                    # Update metadata\n",
        "                    metadata_df.at[idx, 'MaskedPath'] = str(output_path.relative_to(base_path))\n",
        "                    metadata_df.at[idx, 'ROIVisualizationPath'] = str(vis_output_path.relative_to(base_path))\n",
        "\n",
        "                    logger.info(f\"Processed {output_name}\")\n",
        "                finally:\n",
        "                    # Cleanup\n",
        "                    del orig_img, roi_img, result\n",
        "                    gc.collect()\n",
        "            else:\n",
        "                logger.error(f\"Processing failed for {row['Filename']}\")\n",
        "\n",
        "            # Log memory usage periodically\n",
        "            if idx % 5 == 0:\n",
        "                log_memory()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in process_staining_group: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "def process_all_images_from_metadata(drive_folder_path):\n",
        "    \"\"\"Process all images using metadata CSV file.\"\"\"\n",
        "    try:\n",
        "        base_path = Path(drive_folder_path)\n",
        "        metadata_path = base_path / 'metadata.csv'\n",
        "\n",
        "        if not metadata_path.exists():\n",
        "            logger.error(f\"Metadata file not found at: {metadata_path}\")\n",
        "            return None\n",
        "\n",
        "        # Read metadata\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        logger.info(f\"Found {len(metadata_df)} images in metadata\")\n",
        "\n",
        "        # Verify columns\n",
        "        required_columns = ['OriginalPath', 'ROIPath', 'Filename', 'Condition', 'Week', 'Staining', 'Location', 'Animal']\n",
        "        missing_columns = [col for col in required_columns if col not in metadata_df.columns]\n",
        "        if missing_columns:\n",
        "            logger.error(f\"Missing required columns in metadata: {missing_columns}\")\n",
        "            return None\n",
        "\n",
        "        # Create directories\n",
        "        output_dir = base_path / 'Masked-Results'\n",
        "        vis_dir = base_path / 'ROIVisualizations'\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        vis_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Process each staining type\n",
        "        stain_types = metadata_df['Staining'].dropna().unique()\n",
        "        logger.info(f\"Found staining types: {stain_types}\")\n",
        "\n",
        "        for stain in stain_types:\n",
        "            try:\n",
        "                print(f\"\\nNow processing {stain} staining...\")\n",
        "                log_memory()\n",
        "\n",
        "                # Create stain-specific directories\n",
        "                stain_output_dir = output_dir / sanitize_filename(stain)\n",
        "                stain_vis_dir = vis_dir / sanitize_filename(stain)\n",
        "                stain_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "                stain_vis_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Get stain-specific data\n",
        "                stain_df = metadata_df[metadata_df['Staining'] == stain].copy()\n",
        "\n",
        "                # Process staining group\n",
        "                process_staining_group(stain_df, base_path, stain_output_dir, stain_vis_dir, metadata_df)\n",
        "\n",
        "                # Save progress after each staining type\n",
        "                metadata_df.to_csv(metadata_path, index=False)\n",
        "\n",
        "                # Cleanup\n",
        "                del stain_df\n",
        "                gc.collect()\n",
        "                log_memory()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing staining type {stain}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return metadata_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in process_all_images_from_metadata: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the image processing.\"\"\"\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "        except Exception as e:\n",
        "            logger.info(\"Drive already mounted\")\n",
        "\n",
        "        # Get folder path\n",
        "        folder_path = os.getenv('PROCESSED_FOLDER_PATH')\n",
        "        if not folder_path:\n",
        "            logger.error(\"Previous folder path not found! Please run the SVG processing script first.\")\n",
        "            return\n",
        "\n",
        "        base_path = Path(folder_path)\n",
        "        if not base_path.exists() or not (base_path / 'metadata.csv').exists():\n",
        "            logger.error(f\"Metadata file not found in: {folder_path}\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Using previously processed folder: {folder_path}\")\n",
        "\n",
        "        # Process images\n",
        "        processed_results = process_all_images_from_metadata(folder_path)\n",
        "\n",
        "        if processed_results:\n",
        "            logger.info(\"Processing completed successfully!\")\n",
        "        else:\n",
        "            logger.error(\"Processing failed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "197rP6rijXS-"
      },
      "source": [
        "Tiling ROIs using Grid lines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import gc\n",
        "import psutil\n",
        "import platform\n",
        "import time\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def log_memory():\n",
        "    \"\"\"Log current memory usage.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "    logger.info(f\"Memory usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "def enhance_grid_lines(grid_image):\n",
        "    \"\"\"Enhance grid lines using multiple preprocessing techniques.\"\"\"\n",
        "    try:\n",
        "        if len(grid_image.shape) == 3:\n",
        "            gray = cv2.cvtColor(grid_image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = grid_image.copy()\n",
        "\n",
        "        combined = np.zeros_like(gray)\n",
        "\n",
        "        # Process methods one at a time to save memory\n",
        "        # Method 1: Adaptive thresholding\n",
        "        adaptive1 = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY_INV, 11, 2\n",
        "        )\n",
        "        combined = cv2.bitwise_or(combined, adaptive1)\n",
        "        del adaptive1\n",
        "\n",
        "        # Method 2: Different adaptive parameters\n",
        "        adaptive2 = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY_INV, 21, 4\n",
        "        )\n",
        "        combined = cv2.bitwise_or(combined, adaptive2)\n",
        "        del adaptive2\n",
        "\n",
        "        # Method 3: Edge detection\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        combined = cv2.bitwise_or(combined, edges)\n",
        "        del edges\n",
        "\n",
        "        # Method 4: Multiple thresholds\n",
        "        for thresh in [50, 100, 150]:\n",
        "            _, binary = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY_INV)\n",
        "            combined = cv2.bitwise_or(combined, binary)\n",
        "            del binary\n",
        "\n",
        "        del gray\n",
        "        return combined\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in enhance_grid_lines: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def cluster_lines(lines, tolerance=20, filename=None):\n",
        "    \"\"\"Cluster nearby lines.\"\"\"\n",
        "    if not lines:\n",
        "        if filename:\n",
        "            logger.warning(f\"No lines to cluster in file: {filename}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        lines = np.array(sorted(lines))\n",
        "        clusters = []\n",
        "        current_cluster = [lines[0]]\n",
        "\n",
        "        for line in lines[1:]:\n",
        "            if line - current_cluster[-1] <= tolerance:\n",
        "                current_cluster.append(line)\n",
        "            else:\n",
        "                clusters.append(int(np.mean(current_cluster)))\n",
        "                current_cluster = [line]\n",
        "\n",
        "        if current_cluster:\n",
        "            clusters.append(int(np.mean(current_cluster)))\n",
        "\n",
        "        return sorted(clusters)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in cluster_lines: {e}\")\n",
        "        return []\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def detect_grid_lines(grid_image, min_line_length_ratio=0.3, filename=None):\n",
        "    \"\"\"Detect grid lines with adaptive parameters.\"\"\"\n",
        "    try:\n",
        "        height, width = grid_image.shape[:2]\n",
        "        min_line_length = min(height, width) * min_line_length_ratio\n",
        "\n",
        "        # Enhance grid lines\n",
        "        enhanced = enhance_grid_lines(grid_image)\n",
        "        if enhanced is None:\n",
        "            return [], [], None\n",
        "\n",
        "        # Process lines in batches to save memory\n",
        "        h_lines = []\n",
        "        v_lines = []\n",
        "        angle_threshold = 20\n",
        "\n",
        "        for threshold in [50, 100, 150]:\n",
        "            for min_gap in [5, 10, 20]:\n",
        "                lines = cv2.HoughLinesP(\n",
        "                    enhanced,\n",
        "                    rho=1,\n",
        "                    theta=np.pi/180,\n",
        "                    threshold=threshold,\n",
        "                    minLineLength=min_line_length,\n",
        "                    maxLineGap=min_gap\n",
        "                )\n",
        "\n",
        "                if lines is not None:\n",
        "                    for line in lines:\n",
        "                        x1, y1, x2, y2 = line[0]\n",
        "                        angle = abs(np.degrees(np.arctan2(y2 - y1, x2 - x1)))\n",
        "\n",
        "                        if angle < angle_threshold or angle > 180 - angle_threshold:\n",
        "                            h_lines.append((y1 + y2) / 2)\n",
        "                        elif abs(angle - 90) < angle_threshold:\n",
        "                            v_lines.append((x1 + x2) / 2)\n",
        "\n",
        "                    del lines\n",
        "\n",
        "        del enhanced\n",
        "\n",
        "        # Cluster lines\n",
        "        h_clusters = cluster_lines(h_lines, filename=filename)\n",
        "        v_clusters = cluster_lines(v_lines, filename=filename)\n",
        "\n",
        "        # Create debug image\n",
        "        debug_img = cv2.cvtColor(grid_image, cv2.COLOR_BGR2RGB) if len(grid_image.shape) == 3 else cv2.cvtColor(grid_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Draw lines\n",
        "        for y in h_clusters:\n",
        "            cv2.line(debug_img, (0, int(y)), (width, int(y)), (0, 0, 255), 2)\n",
        "        for x in v_clusters:\n",
        "            cv2.line(debug_img, (int(x), 0), (int(x), height), (255, 0, 0), 2)\n",
        "\n",
        "        return h_clusters, v_clusters, debug_img\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in detect_grid_lines for {filename}: {e}\")\n",
        "        return [], [], None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def create_tiles_from_masked(masked_image, h_lines, v_lines):\n",
        "    \"\"\"Create tiles from masked image using detected grid lines.\"\"\"\n",
        "    try:\n",
        "        height, width = masked_image.shape[:2]\n",
        "\n",
        "        # Add boundaries\n",
        "        h_lines = sorted([0] + list(h_lines) + [height])\n",
        "        v_lines = sorted([0] + list(v_lines) + [width])\n",
        "\n",
        "        tiles = []\n",
        "        min_tile_size = 50\n",
        "        min_content_ratio = 0.1\n",
        "\n",
        "        # Process one tile at a time\n",
        "        for i in range(len(h_lines) - 1):\n",
        "            for j in range(len(v_lines) - 1):\n",
        "                y1, y2 = int(h_lines[i]), int(h_lines[i + 1])\n",
        "                x1, x2 = int(v_lines[j]), int(v_lines[j + 1])\n",
        "\n",
        "                # Extract tile\n",
        "                tile = masked_image[y1:y2, x1:x2].copy()\n",
        "\n",
        "                # Check size\n",
        "                if tile.shape[0] < min_tile_size or tile.shape[1] < min_tile_size:\n",
        "                    del tile\n",
        "                    continue\n",
        "\n",
        "                # Check content\n",
        "                if len(tile.shape) == 3:\n",
        "                    non_white = np.any(tile != [255, 255, 255], axis=2)\n",
        "                else:\n",
        "                    non_white = tile != 255\n",
        "\n",
        "                content_ratio = np.mean(non_white)\n",
        "                del non_white\n",
        "\n",
        "                if content_ratio > min_content_ratio:\n",
        "                    tiles.append(tile)\n",
        "                else:\n",
        "                    del tile\n",
        "\n",
        "        return tiles\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in create_tiles_from_masked: {e}\")\n",
        "        return []\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def calculate_roi_area(masked_image, h_lines, v_lines, grid_spacing_microns=500):\n",
        "    \"\"\"Calculate the area of the ROI using grid lines as scale reference.\"\"\"\n",
        "    try:\n",
        "        # Calculate grid sizes\n",
        "        h_spaces = np.diff(sorted(h_lines))\n",
        "        v_spaces = np.diff(sorted(v_lines))\n",
        "\n",
        "        h_median = np.median(h_spaces)\n",
        "        v_median = np.median(v_spaces)\n",
        "\n",
        "        # Calculate valid spaces\n",
        "        valid_h_spaces = h_spaces[abs(h_spaces - h_median) < h_median * 0.5]\n",
        "        valid_v_spaces = v_spaces[abs(v_spaces - v_median) < v_median * 0.5]\n",
        "\n",
        "        avg_grid_size_pixels = np.mean([np.mean(valid_h_spaces), np.mean(valid_v_spaces)])\n",
        "        microns_per_pixel = grid_spacing_microns / avg_grid_size_pixels\n",
        "\n",
        "        # Clean up spacing arrays\n",
        "        del h_spaces, v_spaces, valid_h_spaces, valid_v_spaces\n",
        "\n",
        "        # Create ROI mask\n",
        "        if len(masked_image.shape) == 3:\n",
        "            gray = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = masked_image.copy()\n",
        "\n",
        "        roi_mask = (gray < 255).astype(np.uint8)\n",
        "        del gray\n",
        "\n",
        "        pixel_count = np.count_nonzero(roi_mask)\n",
        "        area_sq_microns = pixel_count * (microns_per_pixel ** 2)\n",
        "\n",
        "        # Create visualization\n",
        "        vis_img = cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        for y in h_lines:\n",
        "            cv2.line(vis_img, (0, int(y)), (vis_img.shape[1], int(y)), (0, 255, 0), 1)\n",
        "        for x in v_lines:\n",
        "            cv2.line(vis_img, (int(x), 0), (int(x), vis_img.shape[0]), (0, 255, 0), 1)\n",
        "\n",
        "        # Handle different OpenCV versions for findContours\n",
        "        findContours_output = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(findContours_output) == 3:\n",
        "            _, contours, _ = findContours_output\n",
        "        else:\n",
        "            contours, _ = findContours_output\n",
        "\n",
        "        cv2.drawContours(vis_img, contours, -1, (255, 0, 0), 2)\n",
        "\n",
        "        del roi_mask, contours\n",
        "\n",
        "        return area_sq_microns, vis_img\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in calculate_roi_area: {e}\")\n",
        "        return None, None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def visualize_tile_process(masked_img, debug_img, tiles, metadata_row, vis_dir):\n",
        "    \"\"\"Visualize original image, grid lines, and all tiles.\"\"\"\n",
        "    try:\n",
        "        plt.close('all')\n",
        "        n_total = 2 + len(tiles)\n",
        "        fig, axs = plt.subplots(1, n_total, figsize=(5 * min(n_total, 10), 5))\n",
        "\n",
        "        # Process one image at a time\n",
        "        rgb_img = cv2.cvtColor(masked_img, cv2.COLOR_BGR2RGB)\n",
        "        axs[0].imshow(rgb_img)\n",
        "        axs[0].set_title('Original Masked Image')\n",
        "        axs[0].axis('off')\n",
        "        del rgb_img\n",
        "\n",
        "        axs[1].imshow(debug_img)\n",
        "        axs[1].set_title('Detected Grid')\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        for idx, tile in enumerate(tiles):\n",
        "            tile_rgb = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
        "            axs[idx + 2].imshow(tile_rgb)\n",
        "            axs[idx + 2].set_title(f'Tile {idx + 1}')\n",
        "            axs[idx + 2].axis('off')\n",
        "            del tile_rgb\n",
        "\n",
        "        plt.tight_layout()\n",
        "        vis_path = vis_dir / f\"{Path(metadata_row['Filename']).stem}_grid_visualization.png\"\n",
        "        plt.savefig(str(vis_path), bbox_inches='tight', dpi=250)\n",
        "\n",
        "        # Clean up plot objects\n",
        "        plt.close(fig)\n",
        "        del fig, axs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in visualization for {metadata_row['Filename']}: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "def process_single_image(masked_path, grid_path, metadata_row, base_path, vis_dir, tiles_dir):\n",
        "    \"\"\"Process a single image and its grid.\"\"\"\n",
        "    try:\n",
        "        # Read images\n",
        "        masked_img = cv2.imread(str(masked_path))\n",
        "        grid_img = cv2.imread(str(grid_path))\n",
        "\n",
        "        if masked_img is None or grid_img is None:\n",
        "            logger.error(f\"Failed to read images for {metadata_row['Filename']}\")\n",
        "            return None\n",
        "\n",
        "        if masked_img.shape[:2] != grid_img.shape[:2]:\n",
        "            logger.error(f\"Size mismatch for {metadata_row['Filename']}\")\n",
        "            return None\n",
        "\n",
        "        # Detect grid lines and get visualization\n",
        "        h_lines, v_lines, debug_img = detect_grid_lines(grid_img, filename=metadata_row['Filename'])\n",
        "        if not h_lines or not v_lines:\n",
        "            return None\n",
        "\n",
        "        # Calculate ROI area\n",
        "        area_sq_microns, area_vis = calculate_roi_area(masked_img, h_lines, v_lines)\n",
        "        if area_sq_microns is None:\n",
        "            return None\n",
        "\n",
        "        del grid_img\n",
        "        gc.collect()\n",
        "\n",
        "        # Save area visualization\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(area_vis)\n",
        "            plt.title(f\"ROI Area Measurement\\nArea: {area_sq_microns/1e6:.2f} mm²\")\n",
        "            plt.axis('off')\n",
        "            plt.savefig(\n",
        "                str(vis_dir / f\"{Path(metadata_row['Filename']).stem}_area_measurement.png\"),\n",
        "                bbox_inches='tight',\n",
        "                dpi=250\n",
        "            )\n",
        "        finally:\n",
        "            plt.close()\n",
        "            del area_vis\n",
        "            gc.collect()\n",
        "\n",
        "        # Create and save tiles\n",
        "        tile_paths = []\n",
        "        tiles = create_tiles_from_masked(masked_img, h_lines, v_lines)\n",
        "\n",
        "        if not tiles:\n",
        "            logger.warning(f\"No valid tiles found for {metadata_row['Filename']}\")\n",
        "            return None\n",
        "\n",
        "        # Save each tile\n",
        "        for tile_idx, tile in enumerate(tiles, 1):\n",
        "            try:\n",
        "                original_name = Path(metadata_row['Filename']).stem\n",
        "                new_name = f\"{original_name} - {tile_idx}.png\"\n",
        "                output_path = tiles_dir / new_name\n",
        "\n",
        "                cv2.imwrite(str(output_path), tile)\n",
        "                tile_paths.append(str(output_path.relative_to(base_path)))\n",
        "            finally:\n",
        "                del tile\n",
        "                gc.collect()\n",
        "\n",
        "        # Create visualization\n",
        "        visualize_tile_process(masked_img, debug_img, tiles, metadata_row, vis_dir)\n",
        "        del debug_img, masked_img\n",
        "        gc.collect()\n",
        "\n",
        "        return tile_paths, area_sq_microns\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {metadata_row['Filename']}: {str(e)}\")\n",
        "        return None\n",
        "    finally:\n",
        "        gc.collect()\n",
        "\n",
        "def process_all_masked_images(drive_folder_path, stain_subset=None):\n",
        "    \"\"\"Process all masked images using metadata with improved memory management.\n",
        "\n",
        "    Args:\n",
        "        drive_folder_path: Path to the drive folder\n",
        "        stain_subset: List of specific stains to process. If None, processes all stains.\n",
        "    \"\"\"\n",
        "    base_path = None\n",
        "    metadata_df = None\n",
        "\n",
        "    try:\n",
        "        # Initial aggressive memory cleanup before starting\n",
        "        plt.close('all')\n",
        "        for _ in range(10):\n",
        "            gc.collect()\n",
        "\n",
        "        if platform.system() != 'Windows':\n",
        "            try:\n",
        "                os.system('sync')\n",
        "                os.system('echo 3 > /proc/sys/vm/drop_caches')\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not perform initial memory cleanup: {e}\")\n",
        "\n",
        "        time.sleep(5)  # Give system time to clean up\n",
        "        log_memory()  # Log initial memory state\n",
        "\n",
        "        base_path = Path(drive_folder_path)\n",
        "        metadata_path = base_path / 'metadata.csv'\n",
        "\n",
        "        # Load metadata with error checking\n",
        "        try:\n",
        "            metadata_df = pd.read_csv(metadata_path)\n",
        "            if metadata_df.empty:\n",
        "                raise ValueError(\"Empty metadata file\")\n",
        "            logger.info(f\"Found {len(metadata_df)} images\")\n",
        "\n",
        "            # Add new columns if they don't exist\n",
        "            new_columns = ['TilePaths', 'TILEVisualizationPath', 'ROIArea_sq_microns']\n",
        "            for col in new_columns:\n",
        "                if col not in metadata_df.columns:\n",
        "                    metadata_df[col] = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading metadata: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Create directories\n",
        "        tiles_dir = base_path / 'Tile-Images'\n",
        "        vis_dir = base_path / 'TILEvisualizations'\n",
        "        tiles_dir.mkdir(exist_ok=True)\n",
        "        vis_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Get stain types based on subset\n",
        "        all_stains = metadata_df['Staining'].unique().tolist()\n",
        "        if stain_subset is not None:\n",
        "            stain_types = [stain for stain in stain_subset if stain in all_stains]\n",
        "        else:\n",
        "            stain_types = all_stains\n",
        "\n",
        "        logger.info(f\"Processing staining types: {stain_types}\")\n",
        "\n",
        "        # Process each staining type\n",
        "        for stain in stain_types:\n",
        "            try:\n",
        "                print(f\"\\nNow processing {stain} staining...\")\n",
        "                log_memory()\n",
        "\n",
        "                # Create stain-specific directories\n",
        "                stain_tiles_dir = tiles_dir / stain\n",
        "                stain_vis_dir = vis_dir / stain\n",
        "                stain_tiles_dir.mkdir(parents=True, exist_ok=True)\n",
        "                stain_vis_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Get stain-specific data efficiently\n",
        "                stain_mask = metadata_df['Staining'] == stain\n",
        "                stain_indices = metadata_df.index[stain_mask].tolist()\n",
        "\n",
        "                # Clear mask to free memory\n",
        "                del stain_mask\n",
        "                gc.collect()\n",
        "\n",
        "                # Process all images in staining group\n",
        "                for idx in tqdm(stain_indices, desc=f\"Processing {stain} images\"):\n",
        "                    try:\n",
        "                        # Get row without copying entire dataframe\n",
        "                        row = metadata_df.loc[idx]\n",
        "\n",
        "                        # Check files exist\n",
        "                        masked_path = base_path / row['MaskedPath']\n",
        "                        grid_path = base_path / row['GridPath']\n",
        "\n",
        "                        if not (masked_path.exists() and grid_path.exists()):\n",
        "                            logger.warning(f\"Missing files for {row['Filename']}\")\n",
        "                            continue\n",
        "\n",
        "                        # Process single image\n",
        "                        result = process_single_image(\n",
        "                            masked_path,\n",
        "                            grid_path,\n",
        "                            row,\n",
        "                            base_path,\n",
        "                            stain_vis_dir,\n",
        "                            stain_tiles_dir\n",
        "                        )\n",
        "\n",
        "                        if result is not None:\n",
        "                            tile_paths, area_sq_microns = result\n",
        "\n",
        "                            # Update metadata\n",
        "                            metadata_df.at[idx, 'TilePaths'] = ';'.join(tile_paths)\n",
        "                            metadata_df.at[idx, 'ROIArea_sq_microns'] = area_sq_microns\n",
        "\n",
        "                            # Save visualization path\n",
        "                            vis_filename = f\"{Path(row['Filename']).stem}_grid_visualization.png\"\n",
        "                            vis_path = stain_vis_dir / vis_filename\n",
        "                            metadata_df.at[idx, 'TILEVisualizationPath'] = str(vis_path.relative_to(base_path))\n",
        "\n",
        "                            logger.info(f\"Processed image with {len(tile_paths)} tiles, area: {area_sq_microns/1e6:.2f} mm²\")\n",
        "\n",
        "                            # Clear temporary variables\n",
        "                            del tile_paths\n",
        "                            del area_sq_microns\n",
        "                            del vis_filename\n",
        "                            del vis_path\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing image {row['Filename']}: {e}\")\n",
        "                        continue\n",
        "                    finally:\n",
        "                        # Clean up resources\n",
        "                        plt.close('all')\n",
        "                        gc.collect()\n",
        "\n",
        "                # Save progress after staining group\n",
        "                metadata_df.to_csv(metadata_path, index=False)\n",
        "                log_memory()\n",
        "\n",
        "                # Aggressive cleanup after staining group\n",
        "                plt.close('all')\n",
        "                for _ in range(10):\n",
        "                    gc.collect()\n",
        "\n",
        "                # Force memory release on Unix systems\n",
        "                if platform.system() != 'Windows':\n",
        "                    try:\n",
        "                        os.system('sync')\n",
        "                        os.system('echo 3 > /proc/sys/vm/drop_caches')\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Could not sync memory: {e}\")\n",
        "\n",
        "                # Add waiting time to allow system to clear memory\n",
        "                time.sleep(5)\n",
        "\n",
        "                # Log memory status after cleanup\n",
        "                log_memory()\n",
        "                if platform.system() != 'Windows':\n",
        "                    try:\n",
        "                        import resource\n",
        "                        maxrss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "                        logger.info(f\"Peak memory usage: {maxrss / 1024:.2f} MB\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Could not log peak memory: {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing staining type {stain}: {e}\")\n",
        "                continue\n",
        "            finally:\n",
        "                # Additional cleanup after each stain type\n",
        "                plt.close('all')\n",
        "                gc.collect()\n",
        "\n",
        "        return metadata_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in process_all_masked_images: {str(e)}\")\n",
        "        return None\n",
        "    finally:\n",
        "        # Final cleanup\n",
        "        plt.close('all')\n",
        "        if 'metadata_df' in locals():\n",
        "            del metadata_df\n",
        "        gc.collect()\n",
        "        log_memory()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the grid processing.\"\"\"\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "        except Exception as e:\n",
        "            logger.info(\"Drive already mounted\")\n",
        "\n",
        "        # Get folder path\n",
        "        folder_path = os.getenv('PROCESSED_FOLDER_PATH')\n",
        "        if not folder_path:\n",
        "            logger.error(\"Previous folder path not found! Please run the SVG processing script first.\")\n",
        "            return\n",
        "\n",
        "        base_path = Path(folder_path)\n",
        "        if not (base_path / 'metadata.csv').exists() or not (base_path / 'Masked-Results').exists():\n",
        "            logger.error(f\"Required files missing in {folder_path}\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Using folder: {folder_path}\")\n",
        "\n",
        "        # Process images\n",
        "        processed_results = process_all_masked_images(folder_path)\n",
        "\n",
        "        if processed_results:\n",
        "            logger.info(\"Processing completed successfully!\")\n",
        "        else:\n",
        "            logger.error(\"Processing failed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mU0j4nq8-RFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdC1OkxYnzbP"
      },
      "source": [
        "# **Step 3:** Staining dependent color segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNk7lWqW1aKT"
      },
      "source": [
        "**Staining is detected and major colors are predefined**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAAnds7dx-po"
      },
      "source": [
        "cluster center is defined and then refined by going over all of the pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY6kKf9vAz59"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import logging\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def log_memory():\n",
        "    \"\"\"Log current memory usage.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "    logger.info(f\"Memory usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "def sanitize_filename(name):\n",
        "    return re.sub(r'[^\\w\\-_]', '_', name)\n",
        "\n",
        "def load_metadata(file_path):\n",
        "    try:\n",
        "        metadata_df = pd.read_csv(file_path)\n",
        "        if metadata_df.empty:\n",
        "            raise ValueError(\"The metadata file is empty.\")\n",
        "        logger.info(f\"Successfully loaded metadata with {len(metadata_df)} rows\")\n",
        "        return metadata_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading metadata: {e}\")\n",
        "        return None\n",
        "\n",
        "def define_distinctive_color_groups():\n",
        "    return {\n",
        "        'HE': {\n",
        "            'Nuclei': [\n",
        "                (81, 44, 109),    # Dark purple\n",
        "                (130, 82, 132),   # Medium purple\n",
        "                (165, 127, 175)   # Light purple\n",
        "            ],\n",
        "            'Cytoplasm/Fibrosis/Muscle': [\n",
        "                (136, 41, 73),    # Dark pink\n",
        "                (209, 83, 145),   # Medium pink\n",
        "                (239, 170, 216)   # Light pink\n",
        "            ],\n",
        "            'Other': [\n",
        "                (210, 149, 191),  # Medium mauve\n",
        "                (235, 140, 198),  # Bright pink\n",
        "                (245, 235, 243)   # Very light pink\n",
        "            ]\n",
        "        },\n",
        "        'Trichrome': {\n",
        "            'Nuclei/Cytoplasm': [\n",
        "                (106, 44, 60),    # Dark red\n",
        "                (142, 59, 75),    # Medium red\n",
        "                (209, 160, 172)   # Light pink\n",
        "            ],\n",
        "            'Fibrosis': [\n",
        "                (102, 98, 114),   # Dark blue-gray\n",
        "                (151, 131, 145),  # Medium gray\n",
        "                (190, 200, 211)   # Light blue-gray\n",
        "            ],\n",
        "            'Muscle': [\n",
        "                (115, 14, 15),    # Dark red\n",
        "                (147, 49, 63),    # Medium red\n",
        "                (180, 97, 111)    # Light red\n",
        "            ],\n",
        "            'Other': [\n",
        "                (214, 193, 205),  # Medium pink\n",
        "                (236, 234, 239),  # Very light gray\n",
        "                (242, 233, 239)   # White-pink\n",
        "            ]\n",
        "        },\n",
        "        'Movats': {\n",
        "            'Nuclei/Elastin': [\n",
        "                (20, 3, 10),      # Almost black\n",
        "                (44, 20, 39),     # Dark purple\n",
        "                (89, 44, 59)      # Medium purple-brown\n",
        "            ],\n",
        "            'Fibrosis': [\n",
        "                (70, 30, 39),     # Dark brown\n",
        "                (144, 95, 82),    # Medium brown\n",
        "                (189, 168, 177)   # Light gray-pink\n",
        "            ],\n",
        "            'Muscle/Cytoplasm': [\n",
        "                (57, 10, 19),     # Dark red\n",
        "                (108, 27, 31),    # Medium red\n",
        "                (147, 82, 99)     # Light red-pink\n",
        "            ],\n",
        "            'Other': [\n",
        "                (161, 130, 140),  # Medium pink\n",
        "                (213, 185, 191),  # Light pink\n",
        "                (243, 237, 237)   # Almost white\n",
        "            ]\n",
        "        },\n",
        "        'IHC': {\n",
        "            'Nuclei': [\n",
        "                (20, 15, 17),     # Almost black\n",
        "                (129, 123, 142),  # Medium gray\n",
        "                (214, 205, 212)   # Light gray\n",
        "            ],\n",
        "            'Target': [\n",
        "                (72, 36, 16),     # Dark brown\n",
        "                (166, 139, 125),  # Medium brown\n",
        "                (234, 196, 170)   # Light brown\n",
        "            ],\n",
        "            'Other': [\n",
        "                (191, 188, 190),  # Medium gray\n",
        "                (229, 221, 220),  # Light gray\n",
        "                (241, 235, 234)   # Almost white\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "def get_color_group(stain):\n",
        "    predefined_groups = define_predefined_color_groups()\n",
        "    ihc_stains = ['Laminin', 'Collagen', 'CD31', 'CD68', 'FSP1', 'Desmin']\n",
        "\n",
        "    if stain in predefined_groups:\n",
        "        return predefined_groups[stain]\n",
        "    elif stain in ihc_stains:\n",
        "        return predefined_groups['IHC']\n",
        "    else:\n",
        "        print(f\"Warning: No predefined color group for {stain}. Using IHC colors.\")\n",
        "        return predefined_groups['IHC']\n",
        "\n",
        "def segment_image(image, color_groups):\n",
        "    \"\"\"Segment image based on color groups.\"\"\"\n",
        "    try:\n",
        "        pixels = image.reshape(-1, 3).astype(np.float64)\n",
        "        distances = np.zeros((len(pixels), len(color_groups)))\n",
        "\n",
        "        for i, colors in enumerate(color_groups.values()):\n",
        "            colors_array = np.array(colors)\n",
        "            distances[:, i] = np.min(np.linalg.norm(pixels[:, np.newaxis] - colors_array, axis=2), axis=1)\n",
        "            del colors_array  # Free memory\n",
        "\n",
        "        white_pixels = np.all(pixels > 240, axis=1)\n",
        "        labels = np.zeros(len(pixels), dtype=int)\n",
        "        non_white_mask = ~white_pixels\n",
        "        labels[non_white_mask] = np.argmin(distances[non_white_mask], axis=1)\n",
        "        labels[white_pixels] = -1\n",
        "\n",
        "        del distances, pixels, white_pixels, non_white_mask  # Free memory\n",
        "        result = labels.reshape(image.shape[:2])\n",
        "        del labels  # Free memory\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image segmentation: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_visualization(image, segmented, white_mask, percentages, color_groups, stain, tile_path, show_percentages=False):\n",
        "    \"\"\"Create visualization of segmentation results.\"\"\"\n",
        "    try:\n",
        "        n_colors = len(color_groups)\n",
        "        fig, axes = plt.subplots(1, n_colors + 1, figsize=(5 * (n_colors + 1), 5))\n",
        "\n",
        "        # Original image\n",
        "        axes[0].imshow(image)\n",
        "        axes[0].set_title(\"Original Tile\")\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Segmented regions\n",
        "        for i, (name, _) in enumerate(color_groups.items()):\n",
        "            segment = np.where(\n",
        "                np.logical_and(segmented[..., np.newaxis] == i,\n",
        "                             ~white_mask[..., np.newaxis]),\n",
        "                image,\n",
        "                [255, 255, 255]\n",
        "            )\n",
        "            axes[i + 1].imshow(segment.astype(np.uint8))\n",
        "            title = f\"{name}\\n({percentages[name]:.1f}%)\"\n",
        "            axes[i + 1].set_title(title)\n",
        "            axes[i + 1].axis('off')\n",
        "            del segment  # Free memory\n",
        "\n",
        "        plt.suptitle(f\"Analysis of {os.path.basename(tile_path)}\\n{stain} Staining\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualization: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "\n",
        "def analyze_tile(tile_path, stain, color_groups, base_path, show_output=False):\n",
        "    \"\"\"Analyze a single tile and return percentages.\"\"\"\n",
        "    try:\n",
        "        full_tile_path = base_path / tile_path\n",
        "        if not full_tile_path.exists():\n",
        "            print(f\"Tile not found: {full_tile_path}\")\n",
        "            return None, None\n",
        "\n",
        "        img = cv2.imread(str(full_tile_path))\n",
        "        if img is None:\n",
        "            print(f\"Failed to read image: {full_tile_path}\")\n",
        "            return None, None\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        del img  # Free memory\n",
        "\n",
        "        segmented = segment_image(img_rgb, color_groups)\n",
        "        if segmented is None:\n",
        "            return None, None\n",
        "\n",
        "        white_mask = np.all(img_rgb > 240, axis=2)\n",
        "        total_valid_pixels = np.sum(~white_mask)\n",
        "\n",
        "        if total_valid_pixels == 0:\n",
        "            print(f\"No valid pixels in tile: {tile_path}\")\n",
        "            return None, None\n",
        "\n",
        "        # Calculate percentages\n",
        "        percentages = {}\n",
        "        for i, (name, _) in enumerate(color_groups.items()):\n",
        "            segment_pixels = np.sum(np.logical_and(segmented == i, ~white_mask))\n",
        "            percentage = (segment_pixels / total_valid_pixels) * 100\n",
        "            percentages[name] = percentage\n",
        "\n",
        "        # Create visualization\n",
        "        fig = create_visualization(img_rgb, segmented, white_mask, percentages,\n",
        "                                 color_groups, stain, tile_path, show_percentages=show_output)\n",
        "\n",
        "        del img_rgb, segmented, white_mask  # Free memory\n",
        "        gc.collect()  # Force garbage collection\n",
        "\n",
        "        return fig, percentages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing tile {tile_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def display_color_palette(color_groups, stain, title):\n",
        "    \"\"\"Display the color palette for a stain type.\"\"\"\n",
        "    try:\n",
        "        fig, ax = plt.subplots(figsize=(10, 2))\n",
        "        for i, (name, colors) in enumerate(color_groups.items()):\n",
        "            for j, color in enumerate(colors):\n",
        "                ax.add_patch(plt.Rectangle((i + j/len(colors), 0),\n",
        "                                        1/len(colors), 1,\n",
        "                                        facecolor=np.array(color)/255))\n",
        "            ax.text(i+0.5, -0.1, name,\n",
        "                   ha='center', va='center', rotation=45)\n",
        "\n",
        "        ax.set_xlim(0, len(color_groups))\n",
        "        ax.set_ylim(-0.5, 1)\n",
        "        ax.axis('off')\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying color palette: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "\n",
        "def process_tiles_for_staining(metadata_df, stain, color_groups, base_path):\n",
        "    \"\"\"Process all tiles for a specific staining type.\"\"\"\n",
        "    print(f\"\\nProcessing {stain} stained tiles:\")\n",
        "    log_memory()\n",
        "\n",
        "    # Create output directory in the base path with staining-specific subfolder\n",
        "    output_dir = base_path / 'Staining-Analysis' / stain\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Display color palette first\n",
        "    print(f\"Displaying color palette for {stain} staining...\")\n",
        "    display_color_palette(color_groups, stain, f\"Color Palette for {stain} Stain\")\n",
        "\n",
        "    # Get relevant rows and create a copy to avoid modifying original\n",
        "    stain_rows = metadata_df[metadata_df['Staining'] == stain].copy()\n",
        "    print(f\"Found {len(stain_rows)} images with {stain} staining\")\n",
        "\n",
        "    # Initialize percentage and SD columns, and create a list to store tile data\n",
        "    tile_data_list = []\n",
        "\n",
        "    for segment_name in color_groups.keys():\n",
        "        mean_col = f\"{stain}_{sanitize_filename(segment_name)}_Percentage\"\n",
        "        sd_col = f\"{stain}_{sanitize_filename(segment_name)}_SD\"\n",
        "        if mean_col not in metadata_df.columns:\n",
        "            metadata_df[mean_col] = np.nan\n",
        "            print(f\"Created column: {mean_col}\")\n",
        "        if sd_col not in metadata_df.columns:\n",
        "            metadata_df[sd_col] = np.nan\n",
        "            print(f\"Created column: {sd_col}\")\n",
        "\n",
        "    # Process each image\n",
        "    for idx, row in tqdm(stain_rows.iterrows(), total=len(stain_rows), desc=f\"Processing {stain} images\"):\n",
        "        try:\n",
        "            if pd.isna(row['TilePaths']):\n",
        "                logger.warning(f\"No tiles found for image {row['Filename']}\")\n",
        "                continue\n",
        "\n",
        "            tile_paths = row['TilePaths'].split(';')\n",
        "            segment_percentages = {name: [] for name in color_groups.keys()}\n",
        "\n",
        "            for tile_path in tile_paths:\n",
        "                fig, percentages = analyze_tile(tile_path, stain, color_groups, base_path, show_output=False)\n",
        "\n",
        "                if percentages:\n",
        "                    # Store individual tile data\n",
        "                    tile_data = {\n",
        "                        'Animal': row['Animal'],\n",
        "                        'Condition': row['Condition'],\n",
        "                        'Week': row['Week'],\n",
        "                        'Location': row['Location'],\n",
        "                        'TilePath': tile_path\n",
        "                    }\n",
        "                    # Add percentages for each segment\n",
        "                    for segment_name, percentage in percentages.items():\n",
        "                        tile_data[f\"{stain}_{sanitize_filename(segment_name)}_Percentage\"] = percentage\n",
        "                    tile_data_list.append(tile_data)\n",
        "\n",
        "                    # Accumulate percentages for mean/SD calculation\n",
        "                    for segment_name, percentage in percentages.items():\n",
        "                        segment_percentages[segment_name].append(percentage)\n",
        "\n",
        "                    # Save visualization\n",
        "                    if fig:\n",
        "                        try:\n",
        "                            base_name = Path(tile_path).stem\n",
        "                            vis_path = output_dir / f\"{base_name}_analysis.png\"\n",
        "                            fig.savefig(str(vis_path), dpi=250, bbox_inches='tight')\n",
        "                        finally:\n",
        "                            plt.close(fig)\n",
        "                            del fig\n",
        "\n",
        "            # Calculate and store mean percentages and SDs\n",
        "            for segment_name, percentage_list in segment_percentages.items():\n",
        "                if percentage_list:\n",
        "                    mean_col = f\"{stain}_{sanitize_filename(segment_name)}_Percentage\"\n",
        "                    sd_col = f\"{stain}_{sanitize_filename(segment_name)}_SD\"\n",
        "                    mean_percentage = np.mean(percentage_list)\n",
        "                    sd_percentage = np.std(percentage_list, ddof=1) if len(percentage_list) > 1 else 0\n",
        "                    metadata_df.at[idx, mean_col] = mean_percentage\n",
        "                    metadata_df.at[idx, sd_col] = sd_percentage\n",
        "\n",
        "            # Clean up after each image\n",
        "            del segment_percentages\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing image {row['Filename']}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Periodically log memory usage\n",
        "        if idx % 10 == 0:\n",
        "            log_memory()\n",
        "\n",
        "    # Save individual tile data\n",
        "    tile_df = pd.DataFrame(tile_data_list)\n",
        "    tile_data_path = base_path / f'{stain}_tile_data.csv'\n",
        "    tile_df.to_csv(tile_data_path, index=False)\n",
        "    print(f\"Saved individual tile data to: {tile_data_path}\")\n",
        "\n",
        "    # Final cleanup\n",
        "    plt.close('all')\n",
        "    gc.collect()\n",
        "    log_memory()\n",
        "\n",
        "    return metadata_df, tile_df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Mount Google Drive if not already mounted\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        logger.info(\"Drive already mounted\")\n",
        "\n",
        "    # Get metadata path from the previously processed folder\n",
        "    folder_path = os.getenv('PROCESSED_FOLDER_PATH')\n",
        "\n",
        "    if not folder_path:\n",
        "        logger.error(\"Previous folder path not found! Please run the SVG processing script first.\")\n",
        "        return\n",
        "\n",
        "    base_path = Path(folder_path)\n",
        "    required_files = [\n",
        "        base_path / 'metadata.csv',\n",
        "        base_path / 'Tile-Images'\n",
        "    ]\n",
        "\n",
        "    if not all(path.exists() for path in required_files):\n",
        "        logger.error(f\"Required files missing in {folder_path}. Please ensure the folder contains metadata.csv and Tiles directory.\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Found required files and directories in: {folder_path}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_path = base_path / 'metadata.csv'\n",
        "    metadata_df = load_metadata(metadata_path)\n",
        "    if metadata_df is None:\n",
        "        return\n",
        "\n",
        "    # Get stain types\n",
        "    stain_types = metadata_df['Staining'].unique()\n",
        "    logger.info(f\"Detected stain types: {stain_types}\")\n",
        "\n",
        "    # Create a dictionary to store all tile DataFrames\n",
        "    all_tile_data = {}\n",
        "\n",
        "    try:\n",
        "        # Process each stain type\n",
        "        for stain in stain_types:\n",
        "            logger.info(f\"\\nProcessing {stain} staining...\")\n",
        "            color_group = get_color_group(stain)\n",
        "\n",
        "            # Process tiles and get both metadata and tile data\n",
        "            metadata_df, tile_df = process_tiles_for_staining(metadata_df, stain, color_group, base_path)\n",
        "\n",
        "            # Store tile data\n",
        "            all_tile_data[stain] = tile_df\n",
        "\n",
        "            # Save progress after each stain type\n",
        "            metadata_df.to_csv(metadata_path, index=False)\n",
        "\n",
        "            # Verify columns were created\n",
        "            stain_cols = [col for col in metadata_df.columns\n",
        "                         if col.startswith(f\"{stain}_\")\n",
        "                         and col.endswith('_Percentage')]\n",
        "            logger.info(f\"Created columns: {stain_cols}\")\n",
        "\n",
        "            # Clean up after each stain type\n",
        "            gc.collect()\n",
        "            log_memory()\n",
        "\n",
        "        # Final save and verification\n",
        "        metadata_df.to_csv(metadata_path, index=False)\n",
        "        logger.info(\"\\nMetadata saved successfully\")\n",
        "\n",
        "        # Save combined tile data\n",
        "        combined_tile_data = pd.concat(all_tile_data.values(), ignore_index=True)\n",
        "        combined_tile_path = base_path / 'all_tile_data.csv'\n",
        "        combined_tile_data.to_csv(combined_tile_path, index=False)\n",
        "        logger.info(f\"\\nSaved combined tile data to: {combined_tile_path}\")\n",
        "\n",
        "        # Print summary of tile data\n",
        "        for stain, tile_df in all_tile_data.items():\n",
        "            logger.info(f\"\\n{stain} tile data summary:\")\n",
        "            logger.info(f\"Number of tiles: {len(tile_df)}\")\n",
        "            logger.info(f\"Number of animals: {tile_df['Animal'].nunique()}\")\n",
        "            logger.info(f\"Number of conditions: {tile_df['Condition'].nunique()}\")\n",
        "            logger.info(f\"Percentage columns: {[col for col in tile_df.columns if 'Percentage' in col]}\")\n",
        "\n",
        "        # Verify percentage columns in metadata\n",
        "        percentage_cols = [col for col in metadata_df.columns\n",
        "                          if col.endswith('_Percentage')]\n",
        "        logger.info(f\"\\nTotal percentage columns in metadata: {len(percentage_cols)}\")\n",
        "        logger.info(f\"Columns: {percentage_cols}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main processing: {e}\")\n",
        "    finally:\n",
        "        plt.close('all')\n",
        "        gc.collect()\n",
        "        log_memory()\n",
        "\n",
        "    return base_path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esQ8Wrk37MID"
      },
      "source": [
        "Quantification using individual tile values and mixed effect model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8glcSIEtise"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def sanitize_filename(name):\n",
        "    \"\"\"Convert a string to a valid filename by replacing invalid characters.\"\"\"\n",
        "    return name.replace('/', '_').replace('\\\\', '_').replace(' ', '_')\n",
        "\n",
        "def perform_statistical_analysis(data, staining, location, segment_name,\n",
        "                               percentage_col, output_dir, safe_location, safe_segment):\n",
        "    \"\"\"Perform mixed effects statistical analysis for each week separately.\"\"\"\n",
        "    weeks = sorted(data['Week'].unique())\n",
        "    results = []\n",
        "\n",
        "    FONT_SIZE = {\n",
        "        'title': 18,\n",
        "        'axes_labels': 16,\n",
        "        'tick_labels': 16,\n",
        "        'legend': 14\n",
        "    }\n",
        "\n",
        "    print(f\"\\nPerforming statistical analysis for {staining} - {location} - {segment_name}\")\n",
        "    print(f\"Number of weeks to analyze: {len(weeks)}\")\n",
        "\n",
        "    # Create comprehensive p-value matrix across all timepoints\n",
        "    # First, get only the groups that actually have data\n",
        "    all_groups = []\n",
        "    for week in weeks:\n",
        "        week_data = data[data['Week'] == week]\n",
        "        conditions = sorted(week_data['Condition'].unique())\n",
        "        for condition in conditions:\n",
        "            if len(week_data[week_data['Condition'] == condition]) > 0:  # Check if there's actual data\n",
        "                all_groups.append(f\"{condition}_W{week}\")\n",
        "\n",
        "    comprehensive_matrix = pd.DataFrame(1.0, index=all_groups, columns=all_groups)\n",
        "\n",
        "    # Add comprehensive analysis results to text output\n",
        "    results.extend([\n",
        "        \"COMPREHENSIVE ANALYSIS ACROSS ALL TIME POINTS\",\n",
        "        \"============================================\",\n",
        "        f\"\\nAnalyzing all combinations for {staining} - {location} - {segment_name}\",\n",
        "        f\"Total number of groups: {len(all_groups)}\",\n",
        "        \"Groups included in analysis:\",\n",
        "        \", \".join(all_groups),\n",
        "        \"\\nPairwise Comparisons:\"\n",
        "    ])\n",
        "\n",
        "    # Perform all pairwise comparisons\n",
        "    from itertools import combinations\n",
        "    pairs = list(combinations(all_groups, 2))\n",
        "\n",
        "    for group1, group2 in pairs:\n",
        "        cond1, week1 = group1.rsplit('_W', 1)\n",
        "        cond2, week2 = group2.rsplit('_W', 1)\n",
        "\n",
        "        values1 = data[(data['Week'] == int(week1)) &\n",
        "                     (data['Condition'] == cond1)][percentage_col]\n",
        "        values2 = data[(data['Week'] == int(week2)) &\n",
        "                     (data['Condition'] == cond2)][percentage_col]\n",
        "\n",
        "        if len(values1) > 0 and len(values2) > 0:  # Only compare if both groups have data\n",
        "            t_stat, p_value = stats.ttest_ind(values1, values2, equal_var=False)\n",
        "            comprehensive_matrix.loc[group1, group2] = p_value\n",
        "            comprehensive_matrix.loc[group2, group1] = p_value\n",
        "            results.append(f\"{group1} vs {group2}: p-value = {p_value:.6f}\")\n",
        "\n",
        "    results.extend([\n",
        "        \"\\nComprehensive P-value Matrix:\",\n",
        "        comprehensive_matrix.to_string(),\n",
        "        \"\\nWEEK-BY-WEEK ANALYSIS\",\n",
        "        \"=====================\"\n",
        "    ])\n",
        "\n",
        "    # Create comprehensive heatmap\n",
        "    plt.figure(figsize=(15, 13))\n",
        "    mask = np.triu(np.ones_like(comprehensive_matrix, dtype=bool), k=1)\n",
        "\n",
        "    sns.heatmap(comprehensive_matrix.astype(float),\n",
        "                mask=mask,\n",
        "                annot=True,\n",
        "                cmap='coolwarm_r',\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                fmt='.6f',\n",
        "                linewidths=0.5,\n",
        "                square=True)\n",
        "\n",
        "    plt.title(f'Comprehensive P-value Analysis\\n{staining} {segment_name} ({location})',\n",
        "             fontsize=FONT_SIZE['title'])\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=FONT_SIZE['tick_labels'])\n",
        "    plt.yticks(rotation=0, fontsize=FONT_SIZE['tick_labels'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save comprehensive heatmap\n",
        "    filename_base = f'{staining}_{safe_location}_{safe_segment}'\n",
        "    comp_heatmap_path = output_dir / f'{filename_base}_comprehensive_pvalues.png'\n",
        "    plt.savefig(str(comp_heatmap_path), dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(str(output_dir / f'{filename_base}_comprehensive_pvalues.svg'),\n",
        "                format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Original week-by-week analysis\n",
        "    for week in weeks:\n",
        "        week_data = data[data['Week'] == week].copy()\n",
        "\n",
        "        # Check if we have any data for this week\n",
        "        if len(week_data) == 0:\n",
        "            print(f\"No data available for Week {week}\")\n",
        "            continue\n",
        "\n",
        "        # Convert data types explicitly\n",
        "        week_data[percentage_col] = pd.to_numeric(week_data[percentage_col], errors='coerce')\n",
        "        week_data = week_data.dropna(subset=[percentage_col])\n",
        "\n",
        "        # Get conditions that have data for this week\n",
        "        available_conditions = week_data['Condition'].unique()\n",
        "        print(f\"\\nAnalyzing Week {week}:\")\n",
        "        print(f\"Number of samples: {len(week_data)}\")\n",
        "        print(f\"Available conditions: {available_conditions}\")\n",
        "\n",
        "        if len(available_conditions) <= 1:\n",
        "            print(f\"Only one or no conditions available for week {week}, skipping statistical analysis\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Ensure correct data types\n",
        "            week_data['Animal'] = week_data['Animal'].astype(str)\n",
        "            week_data['Condition'] = week_data['Condition'].astype('category')\n",
        "\n",
        "            # Fit mixed effects model\n",
        "            model = smf.mixedlm(f\"{percentage_col} ~ Condition\", data=week_data, groups=week_data[\"Animal\"])\n",
        "            model_results = model.fit()\n",
        "\n",
        "            print(\"\\nModel Summary:\")\n",
        "            print(model_results.summary())\n",
        "\n",
        "            # Perform pairwise comparisons only for available conditions\n",
        "            from itertools import combinations\n",
        "            pairs = list(combinations(available_conditions, 2))\n",
        "            p_values = []\n",
        "\n",
        "            print(\"\\nPairwise Comparisons:\")\n",
        "            for pair in pairs:\n",
        "                group1 = week_data[week_data['Condition'] == pair[0]][percentage_col]\n",
        "                group2 = week_data[week_data['Condition'] == pair[1]][percentage_col]\n",
        "\n",
        "                if len(group1) > 0 and len(group2) > 0:  # Only compare if both groups have data\n",
        "                    t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
        "                    p_values.append(p_value)\n",
        "                    print(f\"{pair[0]} vs {pair[1]}: p-value = {p_value:.6f}\")\n",
        "\n",
        "            if p_values:  # Only create heatmap if we have comparisons\n",
        "                # Adjust p-values for multiple comparisons\n",
        "                reject, pvals_corrected, _, _ = multipletests(p_values, method='bonferroni')\n",
        "                p_value_matrix = pd.DataFrame(1.0, index=available_conditions, columns=available_conditions)\n",
        "\n",
        "                for idx, pair in enumerate(pairs):\n",
        "                    p_value = pvals_corrected[idx]\n",
        "                    p_value_matrix.loc[pair[0], pair[1]] = p_value\n",
        "                    p_value_matrix.loc[pair[1], pair[0]] = p_value\n",
        "\n",
        "                # Generate heatmap\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(p_value_matrix.astype(float),\n",
        "                           mask=np.triu(np.ones_like(p_value_matrix, dtype=bool), k=1),\n",
        "                           annot=True, cmap='coolwarm_r',\n",
        "                           vmin=0, vmax=1,\n",
        "                           fmt='.6f',\n",
        "                           linewidths=0.5,\n",
        "                           square=True)\n",
        "\n",
        "                plt.title(f'P-value Heatmap - {staining} {segment_name}\\n({location}) Week {week}',\n",
        "                         fontsize=FONT_SIZE['title'])\n",
        "                plt.xticks(rotation=45, ha='right', fontsize=FONT_SIZE['tick_labels'])\n",
        "                plt.yticks(fontsize=FONT_SIZE['tick_labels'])\n",
        "\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save heatmap\n",
        "                filename_base = f'{staining}_{safe_location}_{safe_segment}'\n",
        "                heatmap_path = output_dir / f'{filename_base}_week{week}_pvalues.png'\n",
        "                plt.savefig(str(heatmap_path), dpi=250, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # Save results\n",
        "                results.extend([\n",
        "                    f\"\\nWeek {week}:\",\n",
        "                    \"Mixed Effects Model Results:\",\n",
        "                    str(model_results.summary()),\n",
        "                    \"\\nPairwise t-tests with Bonferroni correction:\",\n",
        "                    str(p_value_matrix)\n",
        "                ])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in statistical analysis for week {week}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Save statistical results\n",
        "    if results:\n",
        "        filename_base = f'{staining}_{safe_location}_{safe_segment}'\n",
        "        results_path = output_dir / f'{filename_base}_stats.txt'\n",
        "        with open(results_path, 'w') as f:\n",
        "            f.write(f\"Statistical Analysis for {staining} - {location} - {segment_name}\\n\")\n",
        "            f.write(\"\\n\".join(results))\n",
        "\n",
        "\n",
        "def create_staining_plots(metadata_df, staining, output_dir):\n",
        "    plt.rcParams['figure.figsize'] = (15, 10)\n",
        "    plt.rcParams['axes.grid'] = True\n",
        "    plt.rcParams['grid.alpha'] = 0.3\n",
        "    plt.rcParams['grid.linestyle'] = '--'\n",
        "\n",
        "    STANDARD_COLORS = {\n",
        "        'Sham': '#CC0000',\n",
        "        'Ctrl': '#404040',\n",
        "        'Native': '#002B5C',\n",
        "        'Test': '#006B5C',\n",
        "        'Treatment': '#457B9D'\n",
        "    }\n",
        "\n",
        "    available_markers = ['o', 's', '^', 'D', 'v', 'P', 'X', 'p', '*', 'h',\n",
        "                        '+', 'x', '1', '2', '3', '4', '<', '>', 'H', 'd']\n",
        "\n",
        "    tile_data = pd.read_csv(Path(os.getenv('PROCESSED_FOLDER_PATH')) / f'{staining}_tile_data.csv')\n",
        "\n",
        "    stain_data = metadata_df[metadata_df['Staining'] == staining].copy()\n",
        "    locations = sorted(stain_data['Location'].unique())\n",
        "    all_weeks = sorted(stain_data['Week'].unique())\n",
        "    all_conditions = sorted(stain_data['Condition'].unique())\n",
        "\n",
        "    condition_colors = {cond: STANDARD_COLORS.get(cond, '#808080') for cond in all_conditions}\n",
        "    marker_styles = {condition: available_markers[idx % len(available_markers)]\n",
        "                    for idx, condition in enumerate(sorted(all_conditions))}\n",
        "\n",
        "    percentage_columns = [col for col in tile_data.columns\n",
        "                       if col.startswith(f\"{staining}_\") and col.endswith(\"_Percentage\")]\n",
        "\n",
        "    for location in locations:\n",
        "        location_data = tile_data[tile_data['Location'] == location]\n",
        "\n",
        "        for percentage_col in percentage_columns:\n",
        "            segment_name = percentage_col.replace(f\"{staining}_\", \"\").replace(\"_Percentage\", \"\")\n",
        "            safe_location = sanitize_filename(location)\n",
        "            safe_segment = sanitize_filename(segment_name)\n",
        "\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            ax = plt.gca()\n",
        "\n",
        "            plt.grid(True, linestyle='--', alpha=0.3, linewidth=1.5)\n",
        "\n",
        "            valid_weeks = []\n",
        "            week_positions = {}\n",
        "            current_pos = 0\n",
        "\n",
        "            for week in all_weeks:\n",
        "                week_data = location_data[location_data['Week'] == week]\n",
        "                if len(week_data) > 0:\n",
        "                    valid_weeks.append(week)\n",
        "                    week_positions[week] = current_pos\n",
        "                    current_pos += 1\n",
        "\n",
        "            condition_means = {condition: {} for condition in all_conditions}\n",
        "            condition_positions = {condition: {} for condition in all_conditions}\n",
        "\n",
        "            for week in valid_weeks:\n",
        "                week_data = location_data[location_data['Week'] == week]\n",
        "                conditions_in_week = sorted(week_data['Condition'].unique())\n",
        "                n_conditions = len(conditions_in_week)\n",
        "\n",
        "                for condition_idx, condition in enumerate(conditions_in_week):\n",
        "                    condition_data = week_data[week_data['Condition'] == condition]\n",
        "                    values = condition_data[percentage_col]\n",
        "\n",
        "                    if len(values) > 0:\n",
        "                        offset = 0.8 * (condition_idx - (n_conditions-1)/2) / n_conditions\n",
        "                        pos = week_positions[week] + offset\n",
        "\n",
        "                        q1, median, q3 = np.percentile(values, [25, 50, 75])\n",
        "                        iqr = q3 - q1\n",
        "                        whisker_low = max(values.min(), q1 - 1.5 * iqr)\n",
        "                        whisker_high = min(values.max(), q3 + 1.5 * iqr)\n",
        "                        mean = values.mean()\n",
        "\n",
        "                        condition_means[condition][week] = mean\n",
        "                        condition_positions[condition][week] = pos\n",
        "\n",
        "                        base_color = condition_colors[condition]\n",
        "                        rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "                        r, g, b, a = rgba\n",
        "                        box_color = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "                        marker_color = (max(0, r * 0.8), max(0, g * 0.8), max(0, b * 0.8), a)\n",
        "\n",
        "                        # Add jitter to points\n",
        "                        jitter = np.random.normal(0, 0.02, size=len(values))\n",
        "\n",
        "                        # Plot individual points with circles\n",
        "                        plt.scatter(pos + jitter, values,\n",
        "                                  marker='o',\n",
        "                                  color=marker_color,\n",
        "                                  edgecolor='black',\n",
        "                                  linewidth=1,\n",
        "                                  alpha=0.8,\n",
        "                                  s=20,\n",
        "                                  zorder=3)\n",
        "\n",
        "                        # Plot mean with condition-specific marker\n",
        "                        plt.plot([pos], [mean],\n",
        "                                marker=marker_styles[condition],\n",
        "                                color=box_color,\n",
        "                                markerfacecolor=marker_color,\n",
        "                                markeredgecolor='black',\n",
        "                                markeredgewidth=1,\n",
        "                                markersize=12,\n",
        "                                linewidth=2.0,\n",
        "                                alpha=0.9,\n",
        "                                zorder=4)\n",
        "\n",
        "                        box = plt.Rectangle((pos - 0.1, q1), 0.2, q3 - q1,\n",
        "                                         facecolor=box_color,\n",
        "                                         edgecolor='black',\n",
        "                                         linewidth=1,\n",
        "                                         alpha=1,\n",
        "                                         zorder=2)\n",
        "                        ax.add_patch(box)\n",
        "\n",
        "                        plt.hlines(median, pos - 0.1, pos + 0.1,\n",
        "                                 colors='black',\n",
        "                                 linewidth=2.0,\n",
        "                                 zorder=3)\n",
        "\n",
        "                        plt.vlines(pos, whisker_low, whisker_high,\n",
        "                                 colors=box_color,\n",
        "                                 linewidth=2.0,\n",
        "                                 zorder=2)\n",
        "                        plt.hlines([whisker_low, whisker_high],\n",
        "                                 pos - 0.1, pos + 0.1,\n",
        "                                 colors=box_color,\n",
        "                                 linewidth=2.0,\n",
        "                                 zorder=2)\n",
        "\n",
        "            for condition in all_conditions:\n",
        "                weeks_with_data = sorted(condition_means[condition].keys())\n",
        "                for i in range(len(weeks_with_data) - 1):\n",
        "                    current_week = weeks_with_data[i]\n",
        "                    next_week = weeks_with_data[i + 1]\n",
        "\n",
        "                    base_color = condition_colors[condition]\n",
        "                    rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "                    r, g, b, a = rgba\n",
        "                    line_color = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "\n",
        "                    plt.plot([condition_positions[condition][current_week],\n",
        "                            condition_positions[condition][next_week]],\n",
        "                           [condition_means[condition][current_week],\n",
        "                            condition_means[condition][next_week]],\n",
        "                           color=line_color,\n",
        "                           linewidth=2.0,\n",
        "                           alpha=0.9,\n",
        "                           zorder=1)\n",
        "\n",
        "            plt.title(f'{segment_name} Analysis for {staining} Staining - {location}',\n",
        "                     fontsize=30, fontweight='bold')\n",
        "            plt.xlabel('Week', fontsize=28, fontweight='bold')\n",
        "            plt.ylabel(f'{segment_name} Percentage', fontsize=28, fontweight='bold')\n",
        "\n",
        "            plt.xlim(-0.5, len(valid_weeks) - 0.5)\n",
        "            plt.xticks(range(len(valid_weeks)),\n",
        "                      [f'Week {w}' for w in valid_weeks],\n",
        "                      fontsize=24)\n",
        "            plt.yticks(fontsize=24)\n",
        "\n",
        "            handles = []\n",
        "            for condition in all_conditions:\n",
        "                if condition in set().union(*[\n",
        "                    set(location_data[location_data['Week'] == w]['Condition'].unique())\n",
        "                    for w in valid_weeks\n",
        "                ]):\n",
        "                    base_color = condition_colors[condition]\n",
        "                    rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "                    r, g, b, a = rgba\n",
        "                    box_color = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "                    marker_color = (max(0, r * 0.8), max(0, g * 0.8), max(0, b * 0.8), a)\n",
        "\n",
        "                    line = plt.Line2D([0], [0],\n",
        "                                    color=box_color,\n",
        "                                    marker=marker_styles[condition],\n",
        "                                    markerfacecolor=marker_color,\n",
        "                                    markeredgecolor='black',\n",
        "                                    markeredgewidth=1,\n",
        "                                    markersize=12,\n",
        "                                    linewidth=2.0,\n",
        "                                    label=condition)\n",
        "                    handles.append(line)\n",
        "\n",
        "            ax.legend(handles=handles,\n",
        "                     title='Condition',\n",
        "                     loc='upper right',\n",
        "                     fontsize=22,\n",
        "                     title_fontsize=24)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            filename_base = f'{staining}_{safe_location}_{safe_segment}'\n",
        "            plt.savefig(output_dir / f'{filename_base}_analysis.png',\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.savefig(output_dir / f'{filename_base}_analysis.svg',\n",
        "                       format='svg', bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            perform_statistical_analysis(location_data, staining, location,\n",
        "                                      segment_name, percentage_col, output_dir,\n",
        "                                      safe_location, safe_segment)\n",
        "\n",
        "def process_tile_data(metadata_df):\n",
        "\n",
        "    if len(metadata_df) > 0:\n",
        "        print(\"\\nUnique values in key columns:\")\n",
        "        print(f\"Staining types: {metadata_df['Staining'].unique()}\")\n",
        "        print(f\"Conditions: {metadata_df['Condition'].unique()}\")\n",
        "        print(f\"Weeks: {metadata_df['Week'].unique()}\")\n",
        "\n",
        "        percentage_cols = [col for col in metadata_df.columns if 'Percentage' in col]\n",
        "        print(f\"\\nPercentage columns found: {percentage_cols}\")\n",
        "\n",
        "    metadata_df['Week_Group'] = 'Week ' + metadata_df['Week'].astype(str)\n",
        "\n",
        "    return metadata_df\n",
        "\n",
        "def create_area_plots(metadata_df, output_dir):\n",
        "    if 'ROIArea_sq_microns' not in metadata_df.columns:\n",
        "        logger.error(\"ROIArea_sq_microns column not found in metadata\")\n",
        "        return\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = (15, 10)\n",
        "    plt.rcParams['axes.grid'] = True\n",
        "    plt.rcParams['grid.alpha'] = 0.3\n",
        "    plt.rcParams['grid.linestyle'] = '--'\n",
        "\n",
        "    STANDARD_COLORS = {\n",
        "        'Sham': '#CC0000',\n",
        "        'Ctrl': '#404040',\n",
        "        'Native': '#002B5C',\n",
        "        'Test': '#006B5C'\n",
        "    }\n",
        "\n",
        "    available_markers = ['o', 's', '^', 'D', 'v', 'P', 'X', 'p', '*', 'h',\n",
        "                        '+', 'x', '1', '2', '3', '4', '<', '>', 'H', 'd']\n",
        "\n",
        "    stain_data = metadata_df[metadata_df['Staining'] == 'HE'].copy()\n",
        "    if len(stain_data) == 0:\n",
        "        logger.warning(\"No HE staining data found\")\n",
        "        return\n",
        "\n",
        "    stain_data = stain_data[stain_data['Condition'] != 'Native']\n",
        "    staining_dir = output_dir / 'HE'\n",
        "    staining_dir.mkdir(parents=True, exist_ok=True)\n",
        "    stain_data['Area_mm2'] = stain_data['ROIArea_sq_microns'] / 1e6\n",
        "\n",
        "    conditions = sorted(stain_data['Condition'].unique())\n",
        "    weeks = sorted(stain_data['Week'].unique())\n",
        "    condition_colors = {cond: STANDARD_COLORS.get(cond, '#808080') for cond in conditions}\n",
        "    marker_styles = {condition: available_markers[idx % len(available_markers)]\n",
        "                    for idx, condition in enumerate(sorted(conditions))}\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    ax = plt.gca()\n",
        "    plt.grid(True, linestyle='--', alpha=0.3, linewidth=1.5)\n",
        "\n",
        "    valid_weeks = []\n",
        "    week_positions = {}\n",
        "    current_pos = 0\n",
        "\n",
        "    for week in weeks:\n",
        "        week_data = stain_data[stain_data['Week'] == week]\n",
        "        if len(week_data) > 0:\n",
        "            valid_weeks.append(week)\n",
        "            week_positions[week] = current_pos\n",
        "            current_pos += 1\n",
        "\n",
        "    for week in valid_weeks:\n",
        "        week_data = stain_data[stain_data['Week'] == week]\n",
        "        conditions_in_week = sorted(week_data['Condition'].unique())\n",
        "        n_conditions = len(conditions_in_week)\n",
        "\n",
        "        for condition_idx, condition in enumerate(conditions_in_week):\n",
        "            condition_data = week_data[week_data['Condition'] == condition]['Area_mm2']\n",
        "\n",
        "            if len(condition_data) > 0:\n",
        "                offset = 0.8 * (condition_idx - (n_conditions-1)/2) / n_conditions\n",
        "                pos = week_positions[week] + offset\n",
        "\n",
        "                q1, median, q3 = np.percentile(condition_data, [25, 50, 75])\n",
        "                iqr = q3 - q1\n",
        "                whisker_low = max(condition_data.min(), q1 - 1.5 * iqr)\n",
        "                whisker_high = min(condition_data.max(), q3 + 1.5 * iqr)\n",
        "                mean = condition_data.mean()\n",
        "\n",
        "                base_color = condition_colors[condition]\n",
        "                rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "                r, g, b, a = rgba\n",
        "                box_color = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "                marker_color = (max(0, r * 0.8), max(0, g * 0.8), max(0, b * 0.8), a)\n",
        "\n",
        "                # Plot individual points with circles\n",
        "                jitter = np.random.normal(0, 0.02, size=len(condition_data))\n",
        "                plt.scatter(pos + jitter, condition_data,\n",
        "                          marker='o',\n",
        "                          color=marker_color,\n",
        "                          edgecolor='black',\n",
        "                          linewidth=1,\n",
        "                          alpha=0.8,\n",
        "                          s=50,\n",
        "                          zorder=3)\n",
        "\n",
        "                # Plot mean with condition-specific marker\n",
        "                plt.plot([pos], [mean],\n",
        "                        marker=marker_styles[condition],\n",
        "                        color=box_color,\n",
        "                        markerfacecolor=marker_color,\n",
        "                        markeredgecolor='black',\n",
        "                        markeredgewidth=1,\n",
        "                        markersize=12,\n",
        "                        linewidth=2.0,\n",
        "                        alpha=0.9,\n",
        "                        zorder=4)\n",
        "\n",
        "                box = plt.Rectangle((pos - 0.1, q1), 0.2, q3 - q1,\n",
        "                                 facecolor=box_color,\n",
        "                                 edgecolor='black',\n",
        "                                 linewidth=1,\n",
        "                                 alpha=1.0,\n",
        "                                 zorder=2)\n",
        "                ax.add_patch(box)\n",
        "\n",
        "                plt.hlines(median, pos - 0.1, pos + 0.1,\n",
        "                         colors='black',\n",
        "                         linewidth=2.0,\n",
        "                         zorder=3)\n",
        "\n",
        "                plt.vlines(pos, whisker_low, whisker_high,\n",
        "                         colors=box_color,\n",
        "                         linewidth=2.0,\n",
        "                         zorder=2)\n",
        "                plt.hlines([whisker_low, whisker_high],\n",
        "                         pos - 0.1, pos + 0.1,\n",
        "                         colors=box_color,\n",
        "                         linewidth=2.0,\n",
        "                         zorder=2)\n",
        "\n",
        "                if week != valid_weeks[-1]:\n",
        "                    next_week_data = stain_data[\n",
        "                        (stain_data['Week'] == valid_weeks[valid_weeks.index(week) + 1]) &\n",
        "                        (stain_data['Condition'] == condition)\n",
        "                    ]['Area_mm2']\n",
        "                    if len(next_week_data) > 0:\n",
        "                        next_pos = week_positions[valid_weeks[valid_weeks.index(week) + 1]] + offset\n",
        "                        next_mean = next_week_data.mean()\n",
        "                        plt.plot([pos, next_pos], [mean, next_mean],\n",
        "                               color=box_color,\n",
        "                               linewidth=2.0,\n",
        "                               alpha=0.9,\n",
        "                               zorder=1)\n",
        "\n",
        "    locations = sorted(stain_data['Location'].unique())\n",
        "    location_info = f\" ({', '.join(locations)})\" if len(locations) > 1 else \"\"\n",
        "\n",
        "    plt.title(f'ROI Area Analysis for HE Staining{location_info}',\n",
        "             fontsize=30, fontweight='bold')\n",
        "    plt.xlabel('Week', fontsize=28, fontweight='bold')\n",
        "    plt.ylabel('Area (mm²)', fontsize=28, fontweight='bold')\n",
        "\n",
        "    plt.xlim(-0.5, len(valid_weeks) - 0.5)\n",
        "    plt.xticks(range(len(valid_weeks)),\n",
        "              [f'Week {w}' for w in valid_weeks],\n",
        "              fontsize=24)\n",
        "    plt.yticks(fontsize=24)\n",
        "\n",
        "    handles = []\n",
        "    for condition in conditions:\n",
        "        if condition in set().union(*[\n",
        "            set(stain_data[stain_data['Week'] == w]['Condition'].unique())\n",
        "            for w in valid_weeks\n",
        "        ]):\n",
        "            base_color = condition_colors[condition]\n",
        "            rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "            r, g, b, a = rgba\n",
        "            box_color = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "            marker_color = (max(0, r * 0.8), max(0, g * 0.8), max(0, b * 0.8), a)\n",
        "\n",
        "            line = plt.Line2D([0], [0],\n",
        "                            color=box_color,\n",
        "                            marker=marker_styles[condition],\n",
        "                            markerfacecolor=marker_color,\n",
        "                            markeredgecolor='black',\n",
        "                            markeredgewidth=1,\n",
        "                            markersize=12,\n",
        "                            linewidth=2.0,\n",
        "                            label=condition)\n",
        "            handles.append(line)\n",
        "\n",
        "    ax.legend(handles=handles,\n",
        "            title='Condition',\n",
        "            loc='upper right',\n",
        "            fontsize=22,\n",
        "            title_fontsize=24)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(staining_dir / 'HE_area_analysis.png',\n",
        "               dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(staining_dir / 'HE_area_analysis.svg',\n",
        "               format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    perform_statistical_analysis(stain_data, 'HE', \"Combined Locations\", \"Area\",\n",
        "                              'Area_mm2', staining_dir,\n",
        "                              \"Combined\", \"Area\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with Google Drive support.\"\"\"\n",
        "    # Skip mounting if already mounted\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        logger.info(\"Drive already mounted\")\n",
        "\n",
        "    # Get metadata path from the previously processed folder\n",
        "    folder_path = os.getenv('PROCESSED_FOLDER_PATH')\n",
        "\n",
        "    if not folder_path:\n",
        "        logger.error(\"Previous folder path not found! Please run the SVG processing script first.\")\n",
        "        return\n",
        "\n",
        "    base_path = Path(folder_path)\n",
        "    metadata_file = base_path / 'metadata.csv'\n",
        "\n",
        "    if not metadata_file.exists():\n",
        "        logger.error(f\"Metadata file not found in: {folder_path}\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Found metadata.csv in: {folder_path}\")\n",
        "\n",
        "    # Create output directory in the base folder\n",
        "    output_dir = base_path / 'Quantification (Tile-Level)'\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    logger.info(\"Loading metadata...\")\n",
        "    metadata_path = base_path / 'metadata.csv'\n",
        "    metadata_df = pd.read_csv(metadata_path)\n",
        "\n",
        "    logger.info(\"\\nMetadata Overview:\")\n",
        "    logger.info(f\"Total rows: {len(metadata_df)}\")\n",
        "    logger.info(f\"Columns: {metadata_df.columns.tolist()}\")\n",
        "\n",
        "    logger.info(\"\\nChecking for required columns...\")\n",
        "    required_columns = ['TilePaths', 'Staining', 'Condition', 'Week', 'Location']\n",
        "    missing_columns = [col for col in required_columns if col not in metadata_df.columns]\n",
        "    if missing_columns:\n",
        "        logger.warning(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    logger.info(\"\\nCreating area visualizations...\")\n",
        "    create_area_plots(metadata_df, output_dir)\n",
        "\n",
        "    logger.info(\"\\nProcessing tile data...\")\n",
        "    tile_df = process_tile_data(metadata_df)\n",
        "\n",
        "    if len(tile_df) == 0:\n",
        "        logger.error(\"No tile data was processed. Check if TilePaths column contains valid data.\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"\\nCreating plots and performing statistical analysis...\")\n",
        "    for staining in tile_df['Staining'].unique():\n",
        "        logger.info(f\"\\nProcessing {staining} staining...\")\n",
        "\n",
        "        staining_cols = [col for col in tile_df.columns\n",
        "                         if col.startswith(f\"{staining}_\") and col.endswith(\"_Percentage\")]\n",
        "        if not staining_cols:\n",
        "            logger.warning(f\"No percentage columns found for {staining} staining\")\n",
        "            continue\n",
        "\n",
        "        # Create staining type subfolder\n",
        "        staining_dir = output_dir / sanitize_filename(staining)\n",
        "        staining_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        create_staining_plots(tile_df, staining, staining_dir)\n",
        "\n",
        "    logger.info(f\"\\nAnalysis complete. Results saved in {output_dir}\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd92tcxnbs4K"
      },
      "source": [
        "# **Step 4:** Indices definition and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression-based Normalization using Slide level data"
      ],
      "metadata": {
        "id": "6Lr8bxQW4Ad4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_marker_styles(conditions):\n",
        "    \"\"\"Dynamically assign marker styles based on conditions in data.\"\"\"\n",
        "    available_markers = ['o', 's', '^', 'D', 'v', 'P', 'X', 'p', '*', 'h',\n",
        "                         '+', 'x', '1', '2', '3', '4', '<', '>', 'H', 'd']\n",
        "    marker_styles = {}\n",
        "    for idx, condition in enumerate(sorted(conditions)):\n",
        "        marker_styles[condition] = available_markers[idx % len(available_markers)]\n",
        "    return marker_styles\n",
        "\n",
        "\n",
        "def get_marker_colors(markers):\n",
        "    \"\"\"Dynamically assign colors based on markers in data.\"\"\"\n",
        "    fixed_condition_colors = {\n",
        "        'Sham': '#E63946',  # Red\n",
        "        'Ctrl': '#808080',  # Grey\n",
        "        'Native': '#1D3557',  # Blue\n",
        "        'Test': '#2A9D8F'    # Green\n",
        "    }\n",
        "    additional_colors = ['#457B9D', '#E9C46A', '#F4A261', '#9B2226',\n",
        "                         '#005F73', '#AE2012', '#3D405B', '#2B2D42',\n",
        "                         '#8D99AE', '#EF233C', '#4A4E69', '#9A8C98']\n",
        "    marker_colors = {}\n",
        "    additional_idx = 0\n",
        "    for idx, marker in enumerate(sorted(markers)):\n",
        "        if marker in fixed_condition_colors:\n",
        "            marker_colors[marker] = fixed_condition_colors[marker]\n",
        "        else:\n",
        "            marker_colors[marker] = additional_colors[additional_idx % len(additional_colors)]\n",
        "            additional_idx += 1\n",
        "    return marker_colors\n",
        "\n",
        "def create_tpi_tables(he_means, marker_means_dict, nuclei_col, output_dir):\n",
        "    \"\"\"Create detailed tables of the data used in TPI calculations.\"\"\"\n",
        "    output_file = output_dir / 'analysis_summary.txt'\n",
        "    with open(output_file, 'w') as summary_file:\n",
        "        # 1. Create HE Nuclei percentage table\n",
        "        he_table = he_means.pivot_table(\n",
        "            values=nuclei_col,\n",
        "            index=['Location', 'Week'],\n",
        "            columns='Condition',\n",
        "            aggfunc='mean'\n",
        "        ).round(2)\n",
        "        summary_text = \"\\nTPI Analysis Summary Report\\n\"\n",
        "        summary_text += \"==========================\\n\\n\"\n",
        "        summary_text += \"1. HE Nuclei Percentages\\n\"\n",
        "        summary_text += \"-----------------------\\n\"\n",
        "        summary_text += he_table.to_string() + \"\\n\\n\"\n",
        "        print(summary_text)\n",
        "        summary_file.write(summary_text)\n",
        "\n",
        "        # 2. Process marker data\n",
        "        summary_text = \"2. IHC Target Percentages\\n\"\n",
        "        summary_text += \"-----------------------\\n\"\n",
        "        print(summary_text)\n",
        "        summary_file.write(summary_text)\n",
        "        for marker, data in marker_means_dict.items():\n",
        "            marker_table = data['means'].pivot_table(\n",
        "                values=data['target_col'],\n",
        "                index=['Location', 'Week'],\n",
        "                columns='Condition',\n",
        "                aggfunc='mean'\n",
        "            ).round(2)\n",
        "            marker_text = f\"\\n{marker} Target Percentages:\\n\"\n",
        "            marker_text += marker_table.to_string() + \"\\n\\n\"\n",
        "            print(marker_text)\n",
        "            summary_file.write(marker_text)\n",
        "\n",
        "        # 3. Create TPI tables\n",
        "        summary_text = \"3. Target Prevalence Index (TPI) Values\\n\"\n",
        "        summary_text += \"------------------------------------\\n\"\n",
        "        print(summary_text)\n",
        "        summary_file.write(summary_text)\n",
        "        for marker, data in marker_means_dict.items():\n",
        "            tpi_data = []\n",
        "            for location in data['means']['Location'].unique():\n",
        "                for condition in data['means']['Condition'].unique():\n",
        "                    marker_subset = data['means'][\n",
        "                        (data['means']['Location'] == location) &\n",
        "                        (data['means']['Condition'] == condition)\n",
        "                    ]\n",
        "                    he_subset = he_means[\n",
        "                        (he_means['Location'] == location) &\n",
        "                        (he_means['Condition'] == condition)\n",
        "                    ]\n",
        "                    if not marker_subset.empty and not he_subset.empty:\n",
        "                        for week in marker_subset['Week'].unique():\n",
        "                            week_marker = marker_subset[marker_subset['Week'] == week][data['target_col']]\n",
        "                            week_he = he_subset[he_subset['Week'] == week][nuclei_col]\n",
        "                            if not week_marker.empty and not week_he.empty:\n",
        "                                week_tpis = []\n",
        "                                for m in week_marker:\n",
        "                                    for h in week_he:\n",
        "                                        if h != 0:  # Avoid division by zero\n",
        "                                            week_tpis.append(m / h)\n",
        "                                if week_tpis:\n",
        "                                    tpi_data.append({\n",
        "                                        'Location': location,\n",
        "                                        'Condition': condition,\n",
        "                                        'Week': week,\n",
        "                                        'TPI': np.mean(week_tpis),\n",
        "                                        'TPI_SD': np.std(week_tpis, ddof=1) if len(week_tpis) > 1 else 0\n",
        "                                    })\n",
        "            if tpi_data:\n",
        "                tpi_df = pd.DataFrame(tpi_data)\n",
        "                tpi_table = tpi_df.pivot_table(\n",
        "                    values=['TPI', 'TPI_SD'],\n",
        "                    index=['Location', 'Week'],\n",
        "                    columns='Condition',\n",
        "                    aggfunc={'TPI': 'mean', 'TPI_SD': 'mean'}\n",
        "                ).round(3)\n",
        "                tpi_text = f\"\\n{marker} TPI Values:\\n\"\n",
        "                tpi_text += tpi_table.to_string() + \"\\n\\n\"\n",
        "                print(tpi_text)\n",
        "                summary_file.write(tpi_text)\n",
        "\n",
        "                # 4. Add statistics\n",
        "                stats_text = \"4. Statistical Summary\\n\"\n",
        "                stats_text += \"-------------------\\n\"\n",
        "                print(stats_text)\n",
        "                summary_file.write(stats_text)\n",
        "                stats_data = {\n",
        "                    'Max TPI': tpi_df['TPI'].max(),\n",
        "                    'Min TPI': tpi_df['TPI'].min(),\n",
        "                    'Mean TPI': tpi_df['TPI'].mean(),\n",
        "                    'Median TPI': tpi_df['TPI'].median(),\n",
        "                    'Overall Std Dev': tpi_df['TPI'].std(),\n",
        "                    'Average Within-Week Std Dev': tpi_df['TPI_SD'].mean()\n",
        "                }\n",
        "                for stat, value in stats_data.items():\n",
        "                    stat_line = f\"{stat}: {value:.3f}\\n\"\n",
        "                    print(stat_line)\n",
        "                    summary_file.write(stat_line)\n",
        "                print(\"\\n\")\n",
        "                summary_file.write(\"\\n\")\n",
        "\n",
        "def create_marker_plot(marker_data, conditions, unique_weeks, marker, condition_colors, marker_styles, ax):\n",
        "    \"\"\"\n",
        "    Create an improved plot for a single marker with distinct colors for boxes and markers.\n",
        "    Individual lines connect consecutive boxes.\n",
        "    \"\"\"\n",
        "    # Define fixed colors for known conditions\n",
        "    base_colors = {\n",
        "        'Sham': '#CC0000',    # Base red\n",
        "        'Ctrl': '#404040',    # Base grey\n",
        "        'Native': '#002B5C',  # Base blue\n",
        "        'Test': '#006B5C'     # Base green\n",
        "    }\n",
        "\n",
        "    # Fallback colors for other conditions\n",
        "    fallback_colors = ['#2B4B7E', '#B35900', '#8B0000', '#004D40']\n",
        "\n",
        "    # Create color mappings section of the function:\n",
        "    box_colors = {}\n",
        "    marker_colors = {}\n",
        "    fallback_idx = 0\n",
        "\n",
        "    for condition in sorted(conditions):\n",
        "        if condition in base_colors:\n",
        "            base_color = base_colors[condition]\n",
        "        else:\n",
        "            base_color = fallback_colors[fallback_idx % len(fallback_colors)]\n",
        "            fallback_idx += 1\n",
        "\n",
        "        # Convert to RGB for manipulation\n",
        "        rgba = plt.matplotlib.colors.to_rgba(base_color)\n",
        "        r, g, b, a = rgba\n",
        "\n",
        "        # Make boxes lighter by reducing color intensity less (multiply by 1.2 and cap at 1.0)\n",
        "        box_rgba = (min(1.0, r * 1.2), min(1.0, g * 1.2), min(1.0, b * 1.2), a)\n",
        "        box_colors[condition] = box_rgba\n",
        "\n",
        "        # Create darker version for markers (20% darker than base)\n",
        "        darker_rgba = (max(0, r * 0.8), max(0, g * 0.8), max(0, b * 0.8), a)\n",
        "        marker_colors[condition] = darker_rgba\n",
        "\n",
        "    # Plot settings\n",
        "    marker_size = 50\n",
        "    line_width = 2.0\n",
        "    box_width = 0.2\n",
        "    edge_width = 1\n",
        "\n",
        "    # Create week position mapping\n",
        "    week_positions = {week: i for i, week in enumerate(unique_weeks)}\n",
        "    n_conditions = len(conditions)\n",
        "\n",
        "    # Plot for each condition\n",
        "    for condition_idx, condition in enumerate(sorted(conditions)):\n",
        "        # Calculate offset for this condition\n",
        "        offset = 0.8 * (condition_idx - (n_conditions-1)/2) / n_conditions\n",
        "        condition_data = marker_data[marker_data['Condition'] == condition].copy()\n",
        "\n",
        "        if condition_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Add position column for plotting\n",
        "        condition_data['plot_position'] = condition_data['Week'].map(week_positions)\n",
        "\n",
        "        # 1. Plot scatter points\n",
        "        ax.scatter(\n",
        "            [week_positions[week] + offset for week in condition_data['Week']],\n",
        "            condition_data['TPI'],\n",
        "            color=marker_colors[condition],\n",
        "            edgecolor='black',\n",
        "            linewidth=edge_width,\n",
        "            alpha=0.8,\n",
        "            s=marker_size,\n",
        "            zorder=3\n",
        "        )\n",
        "\n",
        "        # 2. Plot boxes for each week\n",
        "        means_data = []\n",
        "        positions = []\n",
        "\n",
        "        for week in unique_weeks:\n",
        "            week_data = condition_data[condition_data['Week'] == week]['TPI']\n",
        "            if not week_data.empty:\n",
        "                current_mean = week_data.mean()\n",
        "                means_data.append(current_mean)\n",
        "                positions.append(week_positions[week] + offset)\n",
        "\n",
        "                ax.boxplot(\n",
        "                    [week_data],\n",
        "                    positions=[week_positions[week] + offset],\n",
        "                    widths=box_width,\n",
        "                    patch_artist=True,\n",
        "                    medianprops={'color': 'black', 'linewidth': line_width},\n",
        "                    boxprops={\n",
        "                        'facecolor': box_colors[condition],\n",
        "                        'alpha': 1.0,\n",
        "                        'linewidth': line_width,\n",
        "                        'edgecolor': 'black'\n",
        "                    },\n",
        "                    whiskerprops={'color': 'black', 'linewidth': line_width},\n",
        "                    capprops={'color': 'black', 'linewidth': line_width},\n",
        "                    flierprops={\n",
        "                        'marker': 'o',\n",
        "                        'markerfacecolor': box_colors[condition],\n",
        "                        'markeredgecolor': 'black',\n",
        "                        'markersize': 4,\n",
        "                        'alpha': 1.0\n",
        "                    },\n",
        "                    zorder=2,\n",
        "                    showfliers=False\n",
        "                )\n",
        "\n",
        "        # 3. Plot individual lines between consecutive means\n",
        "        if len(means_data) > 1:\n",
        "            for i in range(len(means_data) - 1):\n",
        "                # Plot individual line segments\n",
        "                ax.plot(\n",
        "                    [positions[i], positions[i+1]],\n",
        "                    [means_data[i], means_data[i+1]],\n",
        "                    color=box_colors[condition],\n",
        "                    linewidth=line_width,\n",
        "                    alpha=0.9,\n",
        "                    zorder=4\n",
        "                )\n",
        "\n",
        "                # Plot markers at each point\n",
        "                ax.plot(\n",
        "                    positions[i],\n",
        "                    means_data[i],\n",
        "                    color=box_colors[condition],\n",
        "                    marker=marker_styles[condition],\n",
        "                    markerfacecolor=marker_colors[condition],\n",
        "                    markeredgecolor='black',\n",
        "                    markeredgewidth=edge_width,\n",
        "                    markersize=12,\n",
        "                    zorder=5,\n",
        "                    label=condition if i == 0 else \"\"  # Only add label for first point\n",
        "                )\n",
        "\n",
        "            # Plot the last marker separately\n",
        "            ax.plot(\n",
        "                positions[-1],\n",
        "                means_data[-1],\n",
        "                color=box_colors[condition],\n",
        "                marker=marker_styles[condition],\n",
        "                markerfacecolor=marker_colors[condition],\n",
        "                markeredgecolor='black',\n",
        "                markeredgewidth=edge_width,\n",
        "                markersize=12,\n",
        "                zorder=5\n",
        "            )\n",
        "\n",
        "    # Style the plot\n",
        "    ax.set_xlabel('Week', fontsize=28, fontweight='bold')\n",
        "    ax.set_ylabel('Target Prevalence Index (TPI)', fontsize=28, fontweight='bold')\n",
        "    ax.set_title(f'{marker} Prevalence Over Time', fontsize=30, fontweight='bold')\n",
        "    ax.grid(True, linestyle='--', alpha=0.4, linewidth=1.5)\n",
        "\n",
        "    # Set axis properties\n",
        "    ax.set_xticks(range(len(unique_weeks)))\n",
        "    ax.set_xticklabels([f'Week {w}' for w in unique_weeks], fontsize=24)\n",
        "    ax.tick_params(axis='y', labelsize=20)\n",
        "    ax.tick_params(width=2)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(\n",
        "        title='Condition',\n",
        "        loc='upper right',\n",
        "        fontsize=22,\n",
        "        title_fontsize=24\n",
        "    )\n",
        "\n",
        "    return ax\n",
        "\n",
        "def create_tpi_plots(metadata_df, output_dir):\n",
        "    \"\"\"Create Target Prevalence Index (TPI) plots from metadata.\"\"\"\n",
        "    plt.rcParams['figure.figsize'] = (15, 10)\n",
        "    plt.rcParams['axes.grid'] = True\n",
        "    plt.rcParams['grid.alpha'] = 0.3\n",
        "    plt.rcParams['grid.linestyle'] = '--'\n",
        "\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ihc_stains = ['Laminin', 'MHC', 'Collagen', 'Actinin', 'CD31', 'Acetylc',\n",
        "                  'Tubulin', 'CD68', 'FSP1', 'Desmin']\n",
        "\n",
        "    tile_df = metadata_df.copy()\n",
        "    staining_types = sorted(tile_df['Staining'].unique())\n",
        "    ihc_markers = sorted([s for s in staining_types if s in ihc_stains])\n",
        "    conditions = sorted(tile_df['Condition'].unique())\n",
        "    unique_weeks = sorted(tile_df['Week'].unique())\n",
        "\n",
        "    logger.info(f\"Found IHC markers to analyze: {ihc_markers}\")\n",
        "    logger.info(f\"Found conditions: {conditions}\")\n",
        "\n",
        "    marker_styles = get_marker_styles(conditions)\n",
        "    marker_colors = get_marker_colors(ihc_markers)\n",
        "\n",
        "    for condition in conditions:\n",
        "        if condition not in marker_styles:\n",
        "            marker_styles[condition] = 'o'\n",
        "\n",
        "    for marker in ihc_markers:\n",
        "        if marker not in marker_colors:\n",
        "            marker_colors[marker] = '#808080'\n",
        "\n",
        "    he_data = tile_df[tile_df['Staining'] == 'HE'].copy()\n",
        "    nuclei_col = [col for col in he_data.columns if col.startswith('HE_') and 'Nuclei' in col and col.endswith('_Percentage')]\n",
        "\n",
        "    if not nuclei_col:\n",
        "        logger.error(\"No HE Nuclei percentage column found!\")\n",
        "        return\n",
        "    nuclei_col = nuclei_col[0]\n",
        "\n",
        "    he_means = he_data.groupby(['Condition', 'Week', 'Location'])[nuclei_col].mean().reset_index()\n",
        "    marker_means_dict = {}\n",
        "\n",
        "    for marker in ihc_markers:\n",
        "        marker_data = tile_df[tile_df['Staining'] == marker].copy()\n",
        "        if len(marker_data) == 0:\n",
        "            continue\n",
        "\n",
        "        target_col = [col for col in marker_data.columns\n",
        "                     if col.startswith(f'{marker}_Target')\n",
        "                     and col.endswith('_Percentage')]\n",
        "\n",
        "        if not target_col:\n",
        "            logger.warning(f\"No target percentage column found for {marker}\")\n",
        "            continue\n",
        "\n",
        "        target_col = target_col[0]\n",
        "\n",
        "        # Calculate regression-based TPI values\n",
        "        tpi_data = []\n",
        "        for condition in conditions:\n",
        "            for week in unique_weeks:\n",
        "                condition_data = marker_data[\n",
        "                    (marker_data['Condition'] == condition) &\n",
        "                    (marker_data['Week'] == week)\n",
        "                ]\n",
        "\n",
        "                matching_he = he_data[\n",
        "                    (he_data['Condition'] == condition) &\n",
        "                    (he_data['Week'] == week)\n",
        "                ]\n",
        "\n",
        "                if not condition_data.empty and not matching_he.empty:\n",
        "                    # Perform regression for each location\n",
        "                    for location in condition_data['Location'].unique():\n",
        "                        loc_marker = condition_data[condition_data['Location'] == location][target_col].values\n",
        "                        loc_he = matching_he[matching_he['Location'] == location][nuclei_col].values\n",
        "\n",
        "                        if len(loc_marker) > 0 and len(loc_he) > 0:\n",
        "                            # For each condition/week/location combination\n",
        "                            X = loc_he.reshape(-1, 1)  # Independent variable (nuclei)\n",
        "                            y = loc_marker            # Dependent variable (marker)\n",
        "                            reg = LinearRegression()\n",
        "                            reg.fit(X, y)\n",
        "                            residuals = y - reg.predict(X)\n",
        "\n",
        "                            # Append each individual point\n",
        "                            for i in range(len(residuals)):\n",
        "                                tpi_data.append({\n",
        "                                    'Location': location,\n",
        "                                    'Condition': condition,\n",
        "                                    'Week': week,\n",
        "                                    'TPI': residuals[i] + np.mean(y)  # Center around original mean\n",
        "                                })\n",
        "\n",
        "        if tpi_data:\n",
        "            tpi_df = pd.DataFrame(tpi_data)\n",
        "            marker_means = marker_data.groupby(['Location', 'Week', 'Condition'])[target_col].mean().reset_index()\n",
        "            marker_means_dict[marker] = {\n",
        "                'means': marker_means,\n",
        "                'target_col': target_col,\n",
        "                'tpi_data': tpi_df\n",
        "            }\n",
        "            perform_weighted_tpi_statistical_analysis(tpi_df, marker_data, target_col, marker, output_dir)\n",
        "\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            ax = plt.gca()\n",
        "\n",
        "            condition_colors = {\n",
        "                condition: plt.cm.Set2(i/len(conditions))\n",
        "                for i, condition in enumerate(sorted(conditions))\n",
        "            }\n",
        "\n",
        "            create_marker_plot(tpi_df, conditions, unique_weeks, marker,\n",
        "                               condition_colors, marker_styles, ax)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_dir / f'target_prevalence_index_{marker}.png',\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.savefig(output_dir / f'target_prevalence_index_{marker}.svg',\n",
        "                       format='svg', bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    # Create combined plot if needed\n",
        "    if len(marker_means_dict) > 0:\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        ax = plt.gca()\n",
        "        week_positions = {week: i for i, week in enumerate(unique_weeks)}\n",
        "\n",
        "        for marker, data in marker_means_dict.items():\n",
        "            tpi_df = data['tpi_data']\n",
        "\n",
        "            for condition in conditions:\n",
        "                condition_data = tpi_df[tpi_df['Condition'] == condition]\n",
        "                if len(condition_data) > 0:\n",
        "                    means = []\n",
        "                    plot_positions = []\n",
        "\n",
        "                    n_conditions = len(conditions)\n",
        "                    condition_idx = sorted(conditions).index(condition)\n",
        "                    offset = 0.4 * (condition_idx - (n_conditions-1)/2) / n_conditions\n",
        "\n",
        "                    ax.scatter(\n",
        "                        [week_positions[week] + offset for week in condition_data['Week']],\n",
        "                        condition_data['TPI'],\n",
        "                        color=marker_colors[marker],\n",
        "                        alpha=0.4,\n",
        "                        s=20,\n",
        "                        zorder=3\n",
        "                    )\n",
        "\n",
        "                    for week in unique_weeks:\n",
        "                        week_data = condition_data[condition_data['Week'] == week]['TPI']\n",
        "                        if not week_data.empty:\n",
        "                            means.append(week_data.mean())\n",
        "                            plot_positions.append(week_positions[week])\n",
        "\n",
        "                            ax.boxplot(\n",
        "                                [week_data],\n",
        "                                positions=[week_positions[week] + offset],\n",
        "                                widths=0.2,\n",
        "                                patch_artist=True,\n",
        "                                medianprops=dict(color='black'),\n",
        "                                boxprops=dict(facecolor=marker_colors[marker], alpha=0.3),\n",
        "                                whiskerprops=dict(color=marker_colors[marker]),\n",
        "                                capprops=dict(color=marker_colors[marker]),\n",
        "                                flierprops=dict(marker='o', markerfacecolor=marker_colors[marker],\n",
        "                                                markersize=4, alpha=0.5),\n",
        "                                zorder=2,\n",
        "                                showfliers=False\n",
        "                            )\n",
        "\n",
        "                    if means:\n",
        "                        plt.plot([p + offset for p in plot_positions], means,\n",
        "                                 color=marker_colors[marker],\n",
        "                                 marker=marker_styles[condition],\n",
        "                                 markersize=8,\n",
        "                                 linewidth=1.5,\n",
        "                                 alpha=0.7,\n",
        "                                 label=f'{marker} - {condition}',\n",
        "                                 zorder=4)\n",
        "\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "        plt.xlabel('Week', fontsize=16)\n",
        "        plt.ylabel('Target Prevalence Index (TPI)', fontsize=16)\n",
        "        plt.title('Marker Prevalence Over Time', fontsize=18)\n",
        "        plt.xticks(range(len(unique_weeks)), [f'Week {w}' for w in unique_weeks])\n",
        "\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        by_label = dict(zip(labels, handles))\n",
        "        plt.legend(by_label.values(), by_label.keys(),\n",
        "                   bbox_to_anchor=(1.05, 1),\n",
        "                   loc='upper left',\n",
        "                   fontsize=14,\n",
        "                   title='Marker - Condition')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / 'target_prevalence_index.png',\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.savefig(output_dir / 'target_prevalence_index.svg',\n",
        "                   format='svg', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    create_tpi_tables(he_means, marker_means_dict, nuclei_col, output_dir)\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def perform_weighted_tpi_statistical_analysis(tpi_df, marker_data, target_col, marker, output_dir):\n",
        "    \"\"\"\n",
        "    Performs statistical analysis using weighted t-tests with weights from original measurement SDs.\n",
        "    \"\"\"\n",
        "    import statsmodels.api as sm\n",
        "    from scipy import stats\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    FONT_SIZE = {\n",
        "        'title': 18,\n",
        "        'axes_labels': 16,\n",
        "        'tick_labels': 16,\n",
        "        'legend': 14\n",
        "    }\n",
        "\n",
        "    weeks = sorted(tpi_df['Week'].unique())\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\nPerforming TPI statistical analysis for {marker}\")\n",
        "    print(f\"Number of weeks to analyze: {len(weeks)}\")\n",
        "\n",
        "    # Create comprehensive p-value matrix across all timepoints\n",
        "    all_groups = []\n",
        "    for week in weeks:\n",
        "        week_data = tpi_df[tpi_df['Week'] == week]\n",
        "        conditions = sorted(week_data['Condition'].unique())\n",
        "        for condition in conditions:\n",
        "            if len(week_data[week_data['Condition'] == condition]) > 0:\n",
        "                all_groups.append(f\"{condition}_W{week}\")\n",
        "\n",
        "    comprehensive_matrix = pd.DataFrame(1.0, index=all_groups, columns=all_groups)\n",
        "\n",
        "    # Get SD column names\n",
        "    marker_sd_col = target_col.replace('Percentage', 'SD')\n",
        "\n",
        "    results.extend([\n",
        "        \"COMPREHENSIVE ANALYSIS ACROSS ALL TIME POINTS\",\n",
        "        \"============================================\",\n",
        "        f\"\\nAnalyzing all combinations for {marker} TPI\",\n",
        "        f\"Total number of groups: {len(all_groups)}\",\n",
        "        \"Groups included in analysis:\",\n",
        "        \", \".join(all_groups),\n",
        "        \"\\nPairwise Comparisons:\"\n",
        "    ])\n",
        "\n",
        "    from itertools import combinations\n",
        "    pairs = list(combinations(all_groups, 2))\n",
        "\n",
        "    for group1, group2 in pairs:\n",
        "        cond1, week1 = group1.rsplit('_W', 1)\n",
        "        cond2, week2 = group2.rsplit('_W', 1)\n",
        "\n",
        "        # Get TPI values and corresponding weights from marker SDs\n",
        "        group1_tpi = tpi_df[(tpi_df['Week'] == int(week1)) &\n",
        "                           (tpi_df['Condition'] == cond1)]\n",
        "        group2_tpi = tpi_df[(tpi_df['Week'] == int(week2)) &\n",
        "                           (tpi_df['Condition'] == cond2)]\n",
        "\n",
        "        # Get corresponding marker data for weights\n",
        "        group1_marker = marker_data[(marker_data['Week'] == int(week1)) &\n",
        "                                  (marker_data['Condition'] == cond1)]\n",
        "        group2_marker = marker_data[(marker_data['Week'] == int(week2)) &\n",
        "                                  (marker_data['Condition'] == cond2)]\n",
        "\n",
        "        if len(group1_tpi) > 0 and len(group2_tpi) > 0:\n",
        "            # Get weights from marker SDs (inverse of variance)\n",
        "            weights1 = 1 / (group1_marker[marker_sd_col].values ** 2)\n",
        "            weights2 = 1 / (group2_marker[marker_sd_col].values ** 2)\n",
        "\n",
        "            # Calculate weighted means\n",
        "            weighted_mean1 = np.average(group1_tpi['TPI'], weights=weights1)\n",
        "            weighted_mean2 = np.average(group2_tpi['TPI'], weights=weights2)\n",
        "\n",
        "            # Calculate weighted variances\n",
        "            weighted_var1 = np.average((group1_tpi['TPI'] - weighted_mean1) ** 2, weights=weights1)\n",
        "            weighted_var2 = np.average((group2_tpi['TPI'] - weighted_mean2) ** 2, weights=weights2)\n",
        "\n",
        "            # Calculate pooled SE\n",
        "            se = np.sqrt(weighted_var1 + weighted_var2)\n",
        "\n",
        "            if se == 0:\n",
        "                p_value = 1.0\n",
        "            else:\n",
        "                # Calculate t-statistic\n",
        "                t_stat = (weighted_mean1 - weighted_mean2) / se\n",
        "                # Use conservative df = min(n1, n2) - 1\n",
        "                df = min(len(group1_tpi), len(group2_tpi)) - 1\n",
        "                p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n",
        "\n",
        "            comprehensive_matrix.loc[group1, group2] = p_value\n",
        "            comprehensive_matrix.loc[group2, group1] = p_value\n",
        "            results.append(f\"{group1} vs {group2}: p-value = {p_value:.6f}\")\n",
        "\n",
        "    results.extend([\n",
        "        \"\\nComprehensive P-value Matrix:\",\n",
        "        comprehensive_matrix.to_string(),\n",
        "        \"\\nWEEK-BY-WEEK ANALYSIS\",\n",
        "        \"=====================\"\n",
        "    ])\n",
        "\n",
        "    # Create comprehensive heatmap\n",
        "    plt.figure(figsize=(15, 13))\n",
        "    mask = np.triu(np.ones_like(comprehensive_matrix, dtype=bool), k=1)\n",
        "\n",
        "    sns.heatmap(comprehensive_matrix.astype(float),\n",
        "                mask=mask,\n",
        "                annot=True,\n",
        "                cmap='coolwarm_r',\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                fmt='.6f',\n",
        "                linewidths=0.5,\n",
        "                square=True)\n",
        "\n",
        "    plt.title(f'Comprehensive P-value Analysis\\n{marker} TPI',\n",
        "             fontsize=FONT_SIZE['title'])\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=FONT_SIZE['tick_labels'])\n",
        "    plt.yticks(rotation=0, fontsize=FONT_SIZE['tick_labels'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save comprehensive heatmap\n",
        "    comp_heatmap_path = output_dir / f'TPI_{marker}_comprehensive_pvalues.png'\n",
        "    plt.savefig(str(comp_heatmap_path), dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(str(output_dir / f'TPI_{marker}_comprehensive_pvalues.svg'),\n",
        "                format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Save results to a text file\n",
        "    results_path = output_dir / f'TPI_{marker}_comprehensive_stats.txt'\n",
        "    with open(results_path, 'w') as f:\n",
        "        f.write(f\"Statistical Analysis for {marker} TPI Values\\n\")\n",
        "        f.write(\"\\n\".join(results))\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with Google Drive support.\"\"\"\n",
        "    # Skip mounting if already mounted\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        logger.info(\"Drive already mounted\")\n",
        "\n",
        "    # Get metadata path from the previously processed folder\n",
        "    folder_path = os.getenv('PROCESSED_FOLDER_PATH')\n",
        "\n",
        "    if not folder_path:\n",
        "        logger.error(\"Previous folder path not found! Please run the SVG processing script first.\")\n",
        "        return\n",
        "\n",
        "    base_path = Path(folder_path)\n",
        "    metadata_file = base_path / 'metadata.csv'\n",
        "\n",
        "    if not metadata_file.exists():\n",
        "        logger.error(f\"Metadata file not found in: {folder_path}\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Found metadata.csv in: {folder_path}\")\n",
        "\n",
        "    # Create TPI plots output directory\n",
        "    output_dir = base_path / 'TPI-Analysis (Regression of Slide data)'\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    logger.info(\"Loading metadata...\")\n",
        "    metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "    logger.info(\"Creating TPI plots...\")\n",
        "    create_tpi_plots(metadata_df, output_dir)\n",
        "\n",
        "    logger.info(f\"TPI analysis complete. Results saved in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1a7_5Debfv-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "wiEcrrIHbnJY",
        "aSPzR2tELvNa",
        "bdC1OkxYnzbP",
        "COavDXNgeM_E",
        "4dtF6iCQ8Gor"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}